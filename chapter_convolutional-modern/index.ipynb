{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8347b65e",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Modern Convolutional Neural Networks\n",
    ":label:`chap_modern_cnn`\n",
    "\n",
    "Now that we understand the basics of wiring together CNNs, \n",
    "we will take you through a tour of modern CNN architectures.\n",
    "In this chapter, each section corresponds \n",
    "to a significant CNN architecture that was \n",
    "at some point (or currently) the base model\n",
    "upon which many research projects and deployed systems were built.\n",
    "Each of these networks was briefly a dominant architecture \n",
    "and many were winners or runners-up in the ImageNet competition,\n",
    "which has served as a barometer of progress\n",
    "on supervised learning in computer vision since 2010.\n",
    "\n",
    "These models include AlexNet, the first large-scale network deployed \n",
    "to beat conventional computer vision methods on a large-scale vision challenge;\n",
    "the VGG network, which makes use of a number of repeating blocks of elements; the network in network (NiN) which convolves \n",
    "whole neural networks patch-wise over inputs; \n",
    "GoogLeNet, which uses networks with parallel concatenations;\n",
    "residual networks (ResNet), which remain the most popular \n",
    "off-the-shelf architecture in computer vision;\n",
    "and densely connected networks (DenseNet), \n",
    "which are expensive to compute but have set some recent benchmarks.\n",
    "\n",
    "While the idea of *deep* neural networks is quite simple\n",
    "(stack together a bunch of layers),\n",
    "performance can vary wildly across architectures and hyperparameter choices.\n",
    "The neural networks described in this chapter\n",
    "are the product of intuition, a few mathematical insights,\n",
    "and a whole lot of trial and error. \n",
    "We present these models in chronological order,\n",
    "partly to convey a sense of the history\n",
    "so that you can form your own intuitions \n",
    "about where the field is heading \n",
    "and perhaps develop your own architectures.\n",
    "For instance,\n",
    "batch normalization and residual connections described in this chapter have offered two popular ideas for training and designing deep models.\n",
    "\n",
    ":begin_tab:toc\n",
    " - [alexnet](alexnet.ipynb)\n",
    " - [vgg](vgg.ipynb)\n",
    " - [nin](nin.ipynb)\n",
    " - [googlenet](googlenet.ipynb)\n",
    " - [batch-norm](batch-norm.ipynb)\n",
    " - [resnet](resnet.ipynb)\n",
    " - [densenet](densenet.ipynb)\n",
    ":end_tab:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}