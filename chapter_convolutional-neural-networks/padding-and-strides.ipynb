{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Preenchimento e Saltos\n",
    ":label:`sec_padding`\n",
    "\n",
    "\n",
    "\n",
    "No exemplo anterior de :numref:`fig_correlation`,\n",
    "nossa entrada tinha altura e largura de 3\n",
    "e nosso núcleo de convolução tinha altura e largura de 2,\n",
    "produzindo uma representação de saída com dimensão $2\\times2$.\n",
    "Como generalizamos em :numref:`sec_conv_layer`,\n",
    "assumindo que\n",
    "a forma de entrada é $n_h\\times n_w$\n",
    "e a forma do kernel de convolução é $k_h\\times k_w$,\n",
    "então a forma de saída será\n",
    "$(n_h-k_h+1) \\times (n_w-k_w+1)$.\n",
    "Portanto, a forma de saída da camada convolucional\n",
    "é determinada pela forma da entrada\n",
    "e a forma do núcleo de convolução.\n",
    "\n",
    "Em vários casos, incorporamos técnicas,\n",
    "incluindo preenchimento e convoluções com saltos,\n",
    "que afetam o tamanho da saída.\n",
    "Como motivação, note que uma vez que os *kernels* geralmente\n",
    "têm largura e altura maiores que $1$,\n",
    "depois de aplicar muitas convoluções sucessivas,\n",
    "tendemos a acabar com resultados que são\n",
    "consideravelmente menor do que nossa entrada.\n",
    "Se começarmos com uma imagem de $240 \\times 240$ pixels,\n",
    "$10$ camadas de $5 \\times 5$ convoluções\n",
    "reduzem a imagem para $200 \\times 200$ pixels,\n",
    "cortando $30 \\%$ da imagem e com ela\n",
    "obliterando qualquer informação interessante\n",
    "nos limites da imagem original.\n",
    "*Preenchimento* é a ferramenta mais popular para lidar com esse problema.\n",
    "\n",
    "In other cases, we may want to reduce the dimensionality drastically,\n",
    "e.g., if we find the original input resolution to be unwieldy.\n",
    "*Strided convolutions* are a popular technique that can help in these instances.\n",
    "\n",
    "## Preenchimento\n",
    "\n",
    "Conforme descrito acima, um problema complicado ao aplicar camadas convolucionais\n",
    "é que tendemos a perder pixels no perímetro de nossa imagem.\n",
    "Uma vez que normalmente usamos pequenos *kernels*,\n",
    "para qualquer convolução dada,\n",
    "podemos perder apenas alguns pixels,\n",
    "mas isso pode somar conforme aplicamos\n",
    "muitas camadas convolucionais sucessivas.\n",
    "Uma solução direta para este problema\n",
    "é adicionar pixels extras de preenchimento ao redor do limite de nossa imagem de entrada,\n",
    "aumentando assim o tamanho efetivo da imagem.\n",
    "Normalmente, definimos os valores dos pixels extras para zero.\n",
    "Em :numref:`img_conv_pad`, preenchemos uma entrada $3 \\times 3$,\n",
    "aumentando seu tamanho para $5 \\times 5$.\n",
    "A saída correspondente então aumenta para uma matriz $4 \\times 4$\n",
    "As partes sombreadas são o primeiro elemento de saída, bem como os elementos tensores de entrada e kernel usados para o cálculo de saída: $0\\times0+0\\times1+0\\times2+0\\times3=0$.\n",
    "\n",
    "![Correlação cruzada bidimensional com preenchimento.](../img/conv-pad.svg)\n",
    ":label:`img_conv_pad`\n",
    "\n",
    "Em geral, se adicionarmos um total de $p_h$ linhas de preenchimento\n",
    "(cerca de metade na parte superior e metade na parte inferior)\n",
    "e um total de $p_w$ colunas de preenchimento\n",
    "(cerca de metade à esquerda e metade à direita),\n",
    "a forma de saída será\n",
    "\n",
    "$$(n_h-k_h+p_h+1)\\times(n_w-k_w+p_w+1).$$\n",
    "\n",
    "\n",
    "Isso significa que a altura e largura da saída\n",
    "aumentará em $p_h$ e $p_w$, respectivamente.\n",
    "\n",
    "Em muitos casos, queremos definir $p_h=k_h-1$ e $p_w=k_w-1$\n",
    "para dar à entrada e saída a mesma altura e largura.\n",
    "Isso tornará mais fácil prever a forma de saída de cada camada\n",
    "ao construir a rede.\n",
    "Supondo que $k_h$ seja estranho aqui,\n",
    "vamos preencher $p_h/2$ linhas em ambos os lados da altura.\n",
    "Se $k_h$ for par, uma possibilidade é\n",
    "juntar $\\lceil p_h/2\\rceil$ linhas no topo da entrada\n",
    "e $\\lfloor p_h/2\\rfloor$ linhas na parte inferior.\n",
    "Vamos preencher ambos os lados da largura da mesma maneira.\n",
    "\n",
    "\n",
    "CNNs geralmente usam *kernels* de convolução\n",
    "com valores de altura e largura ímpares, como 1, 3, 5 ou 7.\n",
    "Escolher tamanhos ímpares de *kernel* tem o benefício\n",
    "que podemos preservar a dimensionalidade espacial\n",
    "enquanto preenche com o mesmo número de linhas na parte superior e inferior,\n",
    "e o mesmo número de colunas à esquerda e à direita.\n",
    "\n",
    "Além disso, esta prática de usar *kernels* estranhos\n",
    "e preenchimento para preservar precisamente a dimensionalidade\n",
    "oferece um benefício administrativo.\n",
    "Para qualquer tensor bidimensional `X`,\n",
    "quando o tamanho do *kernel* é estranho\n",
    "e o número de linhas e colunas de preenchimento\n",
    "em todos os lados são iguais,\n",
    "produzindo uma saída com a mesma altura e largura da entrada,\n",
    "sabemos que a saída `Y [i, j]` é calculada\n",
    "por correlação cruzada do kernel de entrada e convolução\n",
    "com a janela centralizada em `X [i, j]`.\n",
    "\n",
    "No exemplo a seguir, criamos uma camada convolucional bidimensional\n",
    "com altura e largura de 3\n",
    "e aplique 1 pixel de preenchimento em todos os lados.\n",
    "Dada uma entrada com altura e largura de 8,\n",
    "descobrimos que a altura e a largura da saída também é 8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# We define a convenience function to calculate the convolutional layer. This\n",
    "# function initializes the convolutional layer weights and performs\n",
    "# corresponding dimensionality elevations and reductions on the input and\n",
    "# output\n",
    "def comp_conv2d(conv2d, X):\n",
    "    # Here (1, 1) indicates that the batch size and the number of channels\n",
    "    # are both 1\n",
    "    X = X.reshape((1, 1) + X.shape)\n",
    "    Y = conv2d(X)\n",
    "    # Exclude the first two dimensions that do not interest us: examples and\n",
    "    # channels\n",
    "    return Y.reshape(Y.shape[2:])\n",
    "# Note that here 1 row or column is padded on either side, so a total of 2\n",
    "# rows or columns are added\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)\n",
    "X = torch.rand(size=(8, 8))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "Quando a altura e largura do núcleo de convolução são diferentes,\n",
    "podemos fazer com que a saída e a entrada tenham a mesma altura e largura\n",
    "definindo diferentes números de preenchimento para altura e largura.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we use a convolution kernel with a height of 5 and a width of 3. The\n",
    "# padding numbers on either side of the height and width are 2 and 1,\n",
    "# respectively\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "## Saltos\n",
    "\n",
    "\n",
    "Ao calcular a correlação cruzada,\n",
    "começamos com a janela de convolução\n",
    "no canto superior esquerdo do tensor de entrada,\n",
    "e o deslizamos sobre todos os locais para baixo e para a direita.\n",
    "Nos exemplos anteriores, optamos por deslizar um elemento de cada vez.\n",
    "No entanto, às vezes, seja para eficiência computacional\n",
    "ou porque desejamos reduzir a resolução,\n",
    "movemos nossa janela mais de um elemento por vez,\n",
    "pulando os locais intermediários.\n",
    "\n",
    "Nos referimos ao número de linhas e colunas percorridas por slide como o *salto*.\n",
    "Até agora, usamos saltos de 1, tanto para altura quanto para largura.\n",
    "Às vezes, podemos querer dar um salto maior.\n",
    ":numref:`img_conv_stride` mostra uma operação de correlação cruzada bidimensional\n",
    "com um salto de 3 na vertical e 2 na horizontal.\n",
    "As partes sombreadas são os elementos de saída, bem como os elementos tensores de entrada e *kernel* usados ​​para o cálculo de saída: $0\\times0+0\\times1+1\\times2+2\\times3=8$, $0\\times0+6\\times1+0\\times2+0\\times3=6$.\n",
    "Podemos ver que quando o segundo elemento da primeira coluna é gerado,\n",
    "a janela de convolução desliza três fileiras para baixo.\n",
    "A janela de convolução desliza duas colunas para a direita\n",
    "quando o segundo elemento da primeira linha é gerado.\n",
    "Quando a janela de convolução continua a deslizar duas colunas para a direita na entrada,\n",
    "não há saída porque o elemento de entrada não pode preencher a janela\n",
    "(a menos que adicionemos outra coluna de preenchimento).\n",
    "\n",
    "![Correlação cruzada com passos de 3 e 2 para altura e largura, respectivamente.](../img/conv-stride.svg)\n",
    ":label:`img_conv_stride`\n",
    "\n",
    "Em geral, quando o salto para a altura é $s_h$\n",
    "e a distância para a largura é $s_w$, a forma de saída é\n",
    "\n",
    "$$\\lfloor(n_h-k_h+p_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w-k_w+p_w+s_w)/s_w\\rfloor.$$\n",
    "\n",
    "\n",
    "Se definirmos $p_h=k_h-1$ e $p_w=k_w-1$,\n",
    "então a forma de saída será simplificada para\n",
    "$\\lfloor(n_h+s_h-1)/s_h\\rfloor \\times \\lfloor(n_w+s_w-1)/s_w\\rfloor$.\n",
    "Indo um passo adiante, se a altura e largura de entrada\n",
    "são divisíveis pelos saltos na altura e largura,\n",
    "então a forma de saída será $(n_h/s_h) \\times (n_w/s_w)$.\n",
    "\n",
    "Abaixo, definimos os saltos de altura e largura para 2,\n",
    "reduzindo assim pela metade a altura e a largura da entrada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "A seguir, veremos um exemplo um pouco mais complicado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "Por uma questão de brevidade, quando o número de preenchimento\n",
    "em ambos os lados da altura e largura de entrada são $p_h$ e$p_w$ respectivamente, chamamos o preenchimento $(p_h, p_w)$.\n",
    "Especificamente, quando $p_h = p_w = p$, o preenchimento é $p$.\n",
    "Quando os saltos de altura e largura são $s_h$ e $s_w$, respectivamente,\n",
    "chamamos o salto de $(s_h, s_w)$.\n",
    "Especificamente, quando $s_h = s_w = s$, , o salto é $s$.\n",
    "Por padrão, o preenchimento é 0 e a salto é 1.\n",
    "Na prática, raramente usamos saltos não homogêneos ou preenchimento,\n",
    "ou seja, geralmente temos $p_h = p_w$ e $s_h = s_w$.\n",
    "\n",
    "## Resumo\n",
    "\n",
    "* O preenchimento pode aumentar a altura e a largura da saída. Isso geralmente é usado para dar à saída a mesma altura e largura da entrada.\n",
    "* Os saltos podem reduzir a resolução da saída, por exemplo, reduzindo a altura e largura da saída para apenas $1/n$ da altura e largura da entrada ($n$ é um número inteiro maior que $1$).\n",
    "* Preenchimento e saltos podem ser usados para ajustar a dimensionalidade dos dados de forma eficaz.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Para o último exemplo nesta seção, use matemática para calcular a forma de saída para ver se é consistente com o resultado experimental.\n",
    "1. Experimente outras combinações de preenchimento e saltos nos experimentos desta seção.\n",
    "1. Para sinais de áudio, a que corresponde um salto de 2?\n",
    "1. Quais são os benefícios computacionais de uma salto maior que 1?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 18,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/68)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "<!--stackedit_data:\n",
    "eyJoaXN0b3J5IjpbLTE0NDU4MTAyODcsMjEzMjMzMzgwMiwtND\n",
    "Y5Mjc4NjA3LC0xODk2Nzc1NDk1LC0xNDgzMTY2NjEwLDE1NTY5\n",
    "NTczNDgsMTk3NDUwNDg5MiwtOTA0ODM3MzZdfQ==\n",
    "-->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}