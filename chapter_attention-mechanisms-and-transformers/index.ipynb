{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6162a033",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Attention Mechanisms and Transformers\n",
    ":label:`chap_attention-and-transformers`\n",
    "\n",
    "The optic nerve of a primate's visual system\n",
    "receives massive sensory input,\n",
    "far exceeding what the brain can fully process.\n",
    "Fortunately,\n",
    "not all stimuli are created equal.\n",
    "Focalization and concentration of consciousness\n",
    "have enabled primates to direct attention\n",
    "to objects of interest,\n",
    "such as preys and predators,\n",
    "in the complex visual environment.\n",
    "The ability of paying attention to\n",
    "only a small fraction of the information\n",
    "has evolutionary significance,\n",
    "allowing human beings\n",
    "to live and succeed.\n",
    "\n",
    "Scientists have been studying attention\n",
    "in the cognitive neuroscience field\n",
    "since the 19th century.\n",
    "In this chapter,\n",
    "we will begin by reviewing a popular framework\n",
    "explaining how attention is deployed in a visual scene.\n",
    "Inspired by the attention cues in this framework,\n",
    "we will design models\n",
    "that leverage such attention cues.\n",
    "Notably, the Nadaraya-Watson kernel regression\n",
    "in 1964 is a simple demonstration of machine learning with *attention mechanisms*.\n",
    "Next, we will introduce attention functions\n",
    "that have been extensively used in\n",
    "the design of attention models in deep learning.\n",
    "Specifically,\n",
    "we will show how to use these functions\n",
    "to design the *Bahdanau attention*,\n",
    "a groundbreaking attention model in deep learning\n",
    "that can align bidirectionally and is differentiable.\n",
    "\n",
    "Equipped with\n",
    "the more recent\n",
    "*multi-head attention*\n",
    "and *self-attention* designs,\n",
    "the *transformer* architecture is solely\n",
    "based on attention mechanisms.\n",
    "We will go on to describe its original encoder-decoder design for machine translation.\n",
    "Then we will show how its encoder can\n",
    "represent images, leading to the development of vision transformers.\n",
    "When training very large models on very large datasets (e.g., 300 million images),\n",
    "vision transformers outperform ResNets significantly in image classification, demonstrating superior scalability of transformers.\n",
    "Thus, transformers have been extensively used in large-scale *pretraining*, which can be adapted to perform different tasks with model update (e.g., *fine tuning*) or not (e.g., *few shot*).\n",
    "In the end, we will review how to pretrain transformers as encoder-only (e.g., BERT), encoder-decoder (e.g., T5), and decoder-only (e.g., GPT series).\n",
    "Compelling success of large-scale pretraining with transformers in areas as diverse as\n",
    "language,\n",
    "vision, speech,\n",
    "and reinforcement learning\n",
    "suggests that better performance benefits from larger models, more training data, and more training compute.\n",
    "\n",
    ":begin_tab:toc\n",
    " - [attention-cues](attention-cues.ipynb)\n",
    " - [attention-pooling](attention-pooling.ipynb)\n",
    " - [attention-scoring-functions](attention-scoring-functions.ipynb)\n",
    " - [bahdanau-attention](bahdanau-attention.ipynb)\n",
    " - [multihead-attention](multihead-attention.ipynb)\n",
    " - [self-attention-and-positional-encoding](self-attention-and-positional-encoding.ipynb)\n",
    " - [transformer](transformer.ipynb)\n",
    " - [vision-transformer](vision-transformer.ipynb)\n",
    " - [large-pretraining-transformers](large-pretraining-transformers.ipynb)\n",
    ":end_tab:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}