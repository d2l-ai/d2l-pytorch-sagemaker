{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# GPUs\n",
    ":label:`sec_use_gpu`\n",
    "\n",
    "Em :numref:`tab_intro_decade`, discutimos o rápido crescimento\n",
    "de computação nas últimas duas décadas.\n",
    "Em suma, o desempenho da GPU aumentou\n",
    "por um fator de 1000 a cada década desde 2000.\n",
    "Isso oferece ótimas oportunidades, mas também sugere\n",
    "uma necessidade significativa de fornecer tal desempenho.\n",
    "\n",
    "Nesta seção, começamos a discutir como aproveitar\n",
    "este desempenho computacional para sua pesquisa.\n",
    "Primeiro usando GPUs únicas e, posteriormente,\n",
    "como usar várias GPUs e vários servidores (com várias GPUs).\n",
    "\n",
    "Especificamente, discutiremos como\n",
    "para usar uma única GPU NVIDIA para cálculos.\n",
    "Primeiro, certifique-se de ter pelo menos uma GPU NVIDIA instalada.\n",
    "Em seguida, baixe o [NVIDIA driver e CUDA](https://developer.nvidia.com/cuda-downloads).\n",
    "e siga as instruções para definir o caminho apropriado.\n",
    "Assim que esses preparativos forem concluídos,\n",
    "o comando `nvidia-smi` pode ser usado\n",
    "para ver as informações da placa gráfica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 1,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec 11 06:56:32 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:1B.0 Off |                    0 |\r\n",
      "| N/A   43C    P0    37W / 300W |     11MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   1  Tesla V100-SXM2...  Off  | 00000000:00:1C.0 Off |                    0 |\r\n",
      "| N/A   58C    P0    45W / 300W |     11MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  Tesla V100-SXM2...  Off  | 00000000:00:1D.0 Off |                    0 |\r\n",
      "| N/A   44C    P0    41W / 300W |     11MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  Tesla V100-SXM2...  Off  | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   61C    P0    62W / 300W |     11MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "No PyTorch, cada array possui um dispositivo, frequentemente o referimos como um contexto.\n",
    "Até agora, por padrão, todas as variáveis\n",
    "e computação associada\n",
    "foram atribuídos à CPU.\n",
    "Normalmente, outros contextos podem ser várias GPUs.\n",
    "As coisas podem ficar ainda mais complicadas quando\n",
    "nós implantamos trabalhos em vários servidores.\n",
    "Ao atribuir matrizes a contextos de forma inteligente,\n",
    "podemos minimizar o tempo gasto\n",
    "transferência de dados entre dispositivos.\n",
    "Por exemplo, ao treinar redes neurais em um servidor com uma GPU,\n",
    "normalmente preferimos que os parâmetros do modelo residam na GPU.\n",
    "\n",
    "Em seguida, precisamos confirmar que\n",
    "a versão GPU do PyTorch está instalada.\n",
    "Se uma versão CPU do PyTorch já estiver instalada,\n",
    "precisamos desinstalá-lo primeiro.\n",
    "Por exemplo, use o comando `pip uninstall torch`,\n",
    "em seguida, instale a versão correspondente do PyTorch\n",
    "de acordo com sua versão CUDA.\n",
    "Supondo que você tenha o CUDA 10.0 instalado,\n",
    "você pode instalar a versão PyTorch\n",
    "compatível com CUDA 10.0 via `pip install torch-cu100`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "Para executar os programas desta seção,\n",
    "você precisa de pelo menos duas GPUs.\n",
    "Observe que isso pode ser extravagante para a maioria dos computadores desktop\n",
    "mas está facilmente disponível na nuvem, por exemplo,\n",
    "usando as instâncias multi-GPU do AWS EC2.\n",
    "Quase todas as outras seções * não * requerem várias GPUs.\n",
    "Em vez disso, isso é simplesmente para ilustrar\n",
    "como os dados fluem entre diferentes dispositivos.\n",
    "\n",
    "## Dispositivos Computacionais\n",
    "\n",
    "Podemos especificar dispositivos, como CPUs e GPUs,\n",
    "para armazenamento e cálculo.\n",
    "Por padrão, os tensores são criados na memória principal\n",
    "e, em seguida, use a CPU para calculá-lo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "No PyTorch, a CPU e a GPU podem ser indicadas por `torch.device('cpu')` e `torch.cuda.device('cuda')`.\n",
    "Deve-se notar que o dispositivo `cpu`\n",
    "significa todas as CPUs físicas e memória.\n",
    "Isso significa que os cálculos de PyTorch\n",
    "tentará usar todos os núcleos da CPU.\n",
    "No entanto, um dispositivo `gpu` representa apenas uma placa\n",
    "e a memória correspondente.\n",
    "Se houver várias GPUs, usamos `torch.cuda.device(f'cuda: {i}')`\n",
    "para representar a $i^\\mathrm{th}$ GPU ($i$ começa em 0).\n",
    "Além disso, `gpu:0` e `gpu` são equivalentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'),\n",
       " <torch.cuda.device at 0x7f5ec461f9d0>,\n",
       " <torch.cuda.device at 0x7f5ec4645ca0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.device('cpu'), torch.cuda.device('cuda'), torch.cuda.device('cuda:1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "Podemos consultar o número de GPUs disponíveis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "Agora definimos duas funções convenientes que nos permitem\n",
    "para executar o código mesmo que as GPUs solicitadas não existam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0),\n",
       " device(type='cpu'),\n",
       " [device(type='cuda', index=0), device(type='cuda', index=1)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def try_all_gpus():  #@save\n",
    "    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\n",
    "    devices = [torch.device(f'cuda:{i}')\n",
    "             for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "try_gpu(), try_gpu(10), try_all_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "## Tensores e GPUs\n",
    "\n",
    "Por padrão, tensores são criados na CPU.\n",
    "Podemos consultar o dispositivo onde o tensor está localizado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "origin_pos": 20,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "É importante notar que sempre que quisermos\n",
    "para operar em vários termos,\n",
    "eles precisam estar no mesmo dispositivo.\n",
    "Por exemplo, se somarmos dois tensores,\n",
    "precisamos ter certeza de que ambos os argumentos\n",
    "estão no mesmo dispositivo --- caso contrário, a estrutura\n",
    "não saberia onde armazenar o resultado\n",
    "ou mesmo como decidir onde realizar o cálculo.\n",
    "\n",
    "### Armazenamento na GPU\n",
    "\n",
    "Existem várias maneiras de armazenar um tensor na GPU.\n",
    "Por exemplo, podemos especificar um dispositivo de armazenamento ao criar um tensor.\n",
    "A seguir, criamos a variável tensorial `X` no primeiro `gpu`.\n",
    "O tensor criado em uma GPU consome apenas a memória desta GPU.\n",
    "Podemos usar o comando `nvidia-smi` para ver o uso de memória da GPU.\n",
    "Em geral, precisamos ter certeza de não criar dados que excedam o limite de memória da GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones(2, 3, device=try_gpu())\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 26
   },
   "source": [
    "Supondo que você tenha pelo menos duas GPUs, o código a seguir criará um tensor aleatório na segunda GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "origin_pos": 28,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0183, 0.8929, 0.2991],\n",
       "        [0.5415, 0.1923, 0.6762]], device='cuda:1')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.rand(2, 3, device=try_gpu(1))\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 30
   },
   "source": [
    "### Copiando\n",
    "\n",
    "Se quisermos calcular `X + Y`,\n",
    "precisamos decidir onde realizar esta operação.\n",
    "Por exemplo, como mostrado em :numref:`fig_copyto`,\n",
    "podemos transferir `X` para a segunda GPU\n",
    "e realizar a operação lá.\n",
    "*Não* simplesmente adicione `X` e` Y`,\n",
    "pois isso resultará em uma exceção.\n",
    "O mecanismo de tempo de execução não saberia o que fazer:\n",
    "ele não consegue encontrar dados no mesmo dispositivo e falha.\n",
    "Já que `Y` vive na segunda GPU,\n",
    "precisamos mover `X` para lá antes de podermos adicionar os dois.\n",
    "\n",
    "![Copiar dados para realizar uma operação no mesmo dispositivo.](../img/copyto.svg)\n",
    ":label:`fig_copyto`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "origin_pos": 32,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], device='cuda:0')\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "Z = X.cuda(1)\n",
    "print(X)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 34
   },
   "source": [
    "Agora que os dados estão na mesma GPU\n",
    "(ambos são `Z` e` Y`),\n",
    "podemos somá-los.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "origin_pos": 35,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0183, 1.8929, 1.2991],\n",
       "        [1.5415, 1.1923, 1.6762]], device='cuda:1')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y + Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 37,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "Imagine que sua variável `Z` já esteja em sua segunda GPU.\n",
    "O que acontece se ainda chamarmos `Z.cuda(1)`?\n",
    "Ele retornará `Z` em vez de fazer uma cópia e alocar nova memória.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "origin_pos": 40,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z.cuda(1) is Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 42
   },
   "source": [
    "### Informações extra\n",
    "\n",
    "As pessoas usam GPUs para fazer aprendizado de máquina\n",
    "porque eles esperam que ela seja rápida.\n",
    "Mas a transferência de variáveis entre dispositivos é lenta.\n",
    "Então, queremos que você tenha 100% de certeza\n",
    "que você deseja fazer algo lento antes de deixá-lo fazer.\n",
    "Se a estrutura de *Deep Learning* apenas fizesse a cópia automaticamente\n",
    "sem bater, então você pode não perceber\n",
    "que você escreveu algum código lento.\n",
    "\n",
    "Além disso, a transferência de dados entre dispositivos (CPU, GPUs e outras máquinas)\n",
    "é algo muito mais lento do que a computação.\n",
    "Também torna a paralelização muito mais difícil,\n",
    "já que temos que esperar que os dados sejam enviados (ou melhor, para serem recebidos)\n",
    "antes de prosseguirmos com mais operações.\n",
    "É por isso que as operações de cópia devem ser realizadas com muito cuidado.\n",
    "Como regra geral, muitas pequenas operações\n",
    "são muito piores do que uma grande operação.\n",
    "Além disso, várias operações ao mesmo tempo\n",
    "são muito melhores do que muitas operações simples intercaladas no código\n",
    "a menos que você saiba o que está fazendo.\n",
    "Este é o caso, uma vez que tais operações podem bloquear se um dispositivo\n",
    "tem que esperar pelo outro antes de fazer outra coisa.\n",
    "É um pouco como pedir seu café em uma fila\n",
    "em vez de pré-encomendá-lo por telefone\n",
    "e descobrir que ele está pronto quando você estiver.\n",
    "\n",
    "Por último, quando imprimimos tensores ou convertemos tensores para o formato NumPy,\n",
    "se os dados não estiverem na memória principal,\n",
    "o framework irá copiá-lo para a memória principal primeiro,\n",
    "resultando em sobrecarga de transmissão adicional.\n",
    "Pior ainda, agora está sujeito ao temido bloqueio de intérprete global\n",
    "isso faz tudo esperar que o Python seja concluído.\n",
    "\n",
    "\n",
    "## Redes Neurais e GPUs\n",
    "\n",
    "Da mesma forma, um modelo de rede neural pode especificar dispositivos.\n",
    "O código a seguir coloca os parâmetros do modelo na GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "origin_pos": 44,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(3, 1))\n",
    "net = net.to(device=try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 46
   },
   "source": [
    "Veremos muitos mais exemplos de\n",
    "como executar modelos em GPUs nos capítulos seguintes,\n",
    "simplesmente porque eles se tornarão um pouco mais intensivos em termos de computação.\n",
    "\n",
    "Quando a entrada é um tensor na GPU, o modelo calculará o resultado na mesma GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "origin_pos": 47,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5026],\n",
       "        [1.5026]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 48
   },
   "source": [
    "Vamos confirmar se os parâmetros do modelo estão armazenados na mesma GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "origin_pos": 50,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 52
   },
   "source": [
    "Resumindo, contanto que todos os dados e parâmetros estejam no mesmo dispositivo, podemos aprender modelos com eficiência. Nos próximos capítulos, veremos vários desses exemplos.\n",
    "\n",
    "## Sumário\n",
    "\n",
    "* Podemos especificar dispositivos para armazenamento e cálculo, como CPU ou GPU.\n",
    "   Por padrão, os dados são criados na memória principal\n",
    "   e então usa-se a CPU para cálculos.\n",
    "* A estrutura de *Deep Learning* requer todos os dados de entrada para cálculo\n",
    "   estar no mesmo dispositivo,\n",
    "   seja CPU ou a mesma GPU.\n",
    "* Você pode perder um desempenho significativo movendo dados sem cuidado.\n",
    "   Um erro típico é o seguinte: calcular a perda\n",
    "   para cada minibatch na GPU e relatando de volta\n",
    "   para o usuário na linha de comando (ou registrando-o em um NumPy `ndarray`)\n",
    "   irá disparar um bloqueio global do interpretador que paralisa todas as GPUs.\n",
    "   É muito melhor alocar memória\n",
    "   para registrar dentro da GPU e apenas mover registros maiores.\n",
    "\n",
    "## Exercícios\n",
    "\n",
    "1. Tente uma tarefa de computação maior, como a multiplicação de grandes matrizes,\n",
    "    e veja a diferença de velocidade entre a CPU e a GPU.\n",
    "    Que tal uma tarefa com uma pequena quantidade de cálculos?\n",
    "1. Como devemos ler e escrever os parâmetros do modelo na GPU?\n",
    "1. Meça o tempo que leva para calcular 1000\n",
    "    multiplicações matriz-matriz de $100 \\times 100$ matrizes\n",
    "    e registrar a norma de Frobenius da matriz de saída, um resultado de cada vez\n",
    "    vs. manter um registro na GPU e transferir apenas o resultado final.\n",
    "1. Meça quanto tempo leva para realizar duas multiplicações matriz-matriz\n",
    "    em duas GPUs ao mesmo tempo vs. em sequência\n",
    "    em uma GPU. Dica: você deve ver uma escala quase linear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 54,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussão](https://discuss.d2l.ai/t/63)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 56
   },
   "source": [
    "<!--stackedit_data:\n",
    "eyJoaXN0b3J5IjpbMTM1NzUzMTk4OCwtMTA5NzI3MTAzNiw4MD\n",
    "c2MDQzNTksLTY3MjIyODU4NF19\n",
    "-->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}