{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Processamento de linguagem natural: Pré-treinamento\n",
    ":label:`chap_nlp_pretrain`\n",
    "\n",
    "Os humanos precisam se comunicar.\n",
    "A partir dessa necessidade básica da condição humana, uma vasta quantidade de texto escrito tem sido gerada diariamente.\n",
    "Dado o texto rico em mídia social, aplicativos de chat, e-mails, análises de produtos, artigos de notícias, artigos de pesquisa e livros, torna-se vital permitir que os computadores os entendam para oferecer assistência ou tomar decisões com base em linguagens humanas.\n",
    "\n",
    "O processamento de linguagem natural estuda as interações entre computadores e humanos usando linguagens naturais.\n",
    "Na prática, é muito comum usar técnicas de processamento de linguagem natural para processar e analisar dados de texto (linguagem natural humana), como modelos de linguagem em :numref:`sec_language_model` e modelos de tradução automática em :numref:`sec_machine_translation`.\n",
    "\n",
    "Para entender o texto, podemos começar com sua representação,\n",
    "como tratar cada palavra ou subpalavra como um token de texto individual.\n",
    "Como veremos neste capítulo,\n",
    "a representação de cada token pode ser pré-treinada em um grande corpus,\n",
    "usando word2vec, GloVe ou modelos de incorporação de subpalavra.\n",
    "Após o pré-treinamento, a representação de cada token pode ser um vetor,\n",
    "no entanto, permanece o mesmo, independentemente do contexto.\n",
    "Por exemplo, a representação vetorial de \"banco\" é a mesma\n",
    "em ambos\n",
    "\"vá ao banco para depositar algum dinheiro\"\n",
    "e\n",
    "\"vá ao banco para se sentar\".\n",
    "Assim, muitos modelos de pré-treinamento mais recentes adaptam a representação do mesmo token\n",
    "para contextos diferentes.\n",
    "Entre eles está o BERT, um modelo muito mais profundo baseado no codificador do transformador.\n",
    "Neste capítulo, vamos nos concentrar em como pré-treinar tais representações para texto,\n",
    "como destacado em :numref:`fig_nlp-map-pretrain`.\n",
    "\n",
    "![As representações de texto pré-treinadas podem ser alimentadas para várias arquiteturas de aprendizado profundo para diferentes aplicativos de processamento de linguagem natural downstream. Este capítulo enfoca o pré-treinamento de representação de texto upstream.](../img/nlp-map-pretrain.svg)\n",
    ":label:`fig_nlp-map-pretrain`\n",
    "\n",
    "Conforme mostrado em :numref:`fig_nlp-map-pretrain`,\n",
    "as representações de texto pré-treinadas podem ser alimentadas para\n",
    "uma variedade de arquiteturas de aprendizado profundo para diferentes aplicativos de processamento de linguagem natural downstream.\n",
    "Iremos cobri-los em :numref:`chap_nlp_app`.\n",
    "\n",
    ":begin_tab:toc\n",
    " - [word2vec](word2vec.ipynb)\n",
    " - [approx-training](approx-training.ipynb)\n",
    " - [word-embedding-dataset](word-embedding-dataset.ipynb)\n",
    " - [word2vec-pretraining](word2vec-pretraining.ipynb)\n",
    " - [glove](glove.ipynb)\n",
    " - [subword-embedding](subword-embedding.ipynb)\n",
    " - [similarity-analogy](similarity-analogy.ipynb)\n",
    " - [bert](bert.ipynb)\n",
    " - [bert-dataset](bert-dataset.ipynb)\n",
    " - [bert-pretraining](bert-pretraining.ipynb)\n",
    ":end_tab:\n",
    "\n",
    "<!--stackedit_data:\n",
    "eyJoaXN0b3J5IjpbMTc2MzE4NTcwNSwxMTAzODIwNzIxXX0=\n",
    "-->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}