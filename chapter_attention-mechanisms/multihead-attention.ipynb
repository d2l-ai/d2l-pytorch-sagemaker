{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing (updating) the following libraries for your Sagemaker\n",
    "instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install .. # installing d2l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Atenção Multi-Head \n",
    ":label:`sec_multihead-attention`\n",
    "\n",
    "\n",
    "\n",
    "Na prática,\n",
    "dado o mesmo conjunto de consultas, chaves e valores\n",
    "podemos querer que nosso modelo\n",
    "combine conhecimento de\n",
    "diferentes comportamentos do mesmo mecanismo de atenção,\n",
    "como capturar dependências de vários intervalos (por exemplo, intervalo mais curto vs. intervalo mais longo)\n",
    "dentro de uma sequência.\n",
    "Desse modo,\n",
    "pode ser benéfico\n",
    "permitir nosso mecanismo de atenção\n",
    "para usar em conjunto diferentes subespaços de representação\n",
    "de consultas, chaves e valores.\n",
    "\n",
    "\n",
    "\n",
    "Para este fim,\n",
    "em vez de realizar um único agrupamento de atenção,\n",
    "consultas, chaves e valores\n",
    "podem ser transformados\n",
    "com $h$ projeções lineares aprendidas independentemente.\n",
    "Então, essas $h$ consultas, chaves e valores projetados\n",
    "são alimentados em agrupamento de atenção em paralelo.\n",
    "No fim,\n",
    "$h$ resultados de concentração de atenção\n",
    "são concatenados e\n",
    "transformados com outra projeção linear aprendida\n",
    "para produzir a saída final.\n",
    "Este design\n",
    "é chamado de *atenção multi-head*,\n",
    "onde cada uma das saídas de concentração de $h$\n",
    "é um *head* :cite:`Vaswani.Shazeer.Parmar.ea.2017`.\n",
    "Usando camadas totalmente conectadas\n",
    "para realizar transformações lineares que podem ser aprendidas,\n",
    ":numref:`fig_multi-head-attention`\n",
    "descreve a atenção de *multi-head*.\n",
    "\n",
    "![Multi-head attention, where multiple heads are concatenated then linearly transformed.](../img/multi-head-attention.svg)\n",
    ":label:`fig_multi-head-attention`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Modelo\n",
    "\n",
    "Antes de fornecer a implementação da atenção *multi-head*,\n",
    "vamos formalizar este modelo matematicamente.\n",
    "Dada uma consulta $\\mathbf{q} \\in \\mathbb{R}^{d_q}$,\n",
    "uma chave $\\mathbf{k} \\in \\mathbb{R}^{d_k}$,\n",
    "e um valor $\\mathbf{v} \\in \\mathbb{R}^{d_v}$,\n",
    "cada *head* de atenção $\\mathbf{h}_i$  ($i = 1, \\ldots, h$)\n",
    "é calculado como\n",
    "\n",
    "$$\\mathbf{h}_i = f(\\mathbf W_i^{(q)}\\mathbf q, \\mathbf W_i^{(k)}\\mathbf k,\\mathbf W_i^{(v)}\\mathbf v) \\in \\mathbb R^{p_v},$$\n",
    "\n",
    "onde parâmetros aprendíveis\n",
    "$\\mathbf W_i^{(q)}\\in\\mathbb R^{p_q\\times d_q}$,\n",
    "$\\mathbf W_i^{(k)}\\in\\mathbb R^{p_k\\times d_k}$\n",
    "e $\\mathbf W_i^{(v)}\\in\\mathbb R^{p_v\\times d_v}$,\n",
    "e\n",
    "$f$ é concentração de atenção,\n",
    "tal como\n",
    "atenção aditiva e atenção de produto escalonado\n",
    "em :numref:`sec_attention-scoring-functions`.\n",
    "A saída de atenção *multi-head*\n",
    "é outra transformação linear via\n",
    "parâmetros aprendíveis\n",
    "$\\mathbf W_o\\in\\mathbb R^{p_o\\times h p_v}$\n",
    "da concatenação de $h$ cabeças:\n",
    "\n",
    "$$\\mathbf W_o \\begin{bmatrix}\\mathbf h_1\\\\\\vdots\\\\\\mathbf h_h\\end{bmatrix} \\in \\mathbb{R}^{p_o}.$$\n",
    "\n",
    "Com base neste design,\n",
    "cada cabeça pode atender a diferentes partes da entrada.\n",
    "Funções mais sofisticadas do que a média ponderada simples\n",
    "podem ser expressadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 3
   },
   "source": [
    "## Implementação\n",
    "\n",
    "Em nossa implementação,\n",
    "nós escolhemos a atenção do produto escalonado\n",
    "para cada *head* da atenção de várias cabeças.\n",
    "Para evitar um crescimento significativo\n",
    "de custo computacional e custo de parametrização,\n",
    "montamos\n",
    "$p_q = p_k = p_v = p_o / h$.\n",
    "Observe que $h$ *heads*\n",
    "pode ser calculado em paralelo\n",
    "se definirmos\n",
    "o número de saídas de transformações lineares\n",
    "para a consulta, chave e valor\n",
    "a $p_q h = p_k h = p_v h = p_o$.\n",
    "Na implementação a seguir,\n",
    "$p_o$ é especificado através do argumento `num_hiddens`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 5,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 num_heads, dropout, bias=False, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = d2l.DotProductAttention(dropout)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)\n",
    "        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)\n",
    "        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        # Shape of `queries`, `keys`, or `values`:\n",
    "        # (`batch_size`, no. of queries or key-value pairs, `num_hiddens`)\n",
    "        # Shape of `valid_lens`:\n",
    "        # (`batch_size`,) or (`batch_size`, no. of queries)\n",
    "        # After transposing, shape of output `queries`, `keys`, or `values`:\n",
    "        # (`batch_size` * `num_heads`, no. of queries or key-value pairs,\n",
    "        # `num_hiddens` / `num_heads`)\n",
    "        queries = transpose_qkv(self.W_q(queries), self.num_heads)\n",
    "        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n",
    "        values = transpose_qkv(self.W_v(values), self.num_heads)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            # On axis 0, copy the first item (scalar or vector) for\n",
    "            # `num_heads` times, then copy the next item, and so on\n",
    "            valid_lens = torch.repeat_interleave(\n",
    "                valid_lens, repeats=self.num_heads, dim=0)\n",
    "\n",
    "        # Shape of `output`: (`batch_size` * `num_heads`, no. of queries,\n",
    "        # `num_hiddens` / `num_heads`)\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "\n",
    "        # Shape of `output_concat`:\n",
    "        # (`batch_size`, no. of queries, `num_hiddens`)\n",
    "        output_concat = transpose_output(output, self.num_heads)\n",
    "        return self.W_o(output_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "Para permitir o cálculo paralelo de várias *heads*\n",
    "a classe `MultiHeadAttention` acima usa duas funções de transposição, conforme definido abaixo.\n",
    "Especificamente,\n",
    "a função `transpose_output` reverte a operação\n",
    "da função `transpose_qkv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def transpose_qkv(X, num_heads):\n",
    "    # Shape of input `X`:\n",
    "    # (`batch_size`, no. of queries or key-value pairs, `num_hiddens`).\n",
    "    # Shape of output `X`:\n",
    "    # (`batch_size`, no. of queries or key-value pairs, `num_heads`,\n",
    "    # `num_hiddens` / `num_heads`)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "\n",
    "    # Shape of output `X`:\n",
    "    # (`batch_size`, `num_heads`, no. of queries or key-value pairs,\n",
    "    # `num_hiddens` / `num_heads`)\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "\n",
    "    # Shape of `output`:\n",
    "    # (`batch_size` * `num_heads`, no. of queries or key-value pairs,\n",
    "    # `num_hiddens` / `num_heads`)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "\n",
    "#@save\n",
    "def transpose_output(X, num_heads):\n",
    "    \"\"\"Reverse the operation of `transpose_qkv`\"\"\"\n",
    "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "Vamos testar nossa classe `MultiHeadAttention` implementada\n",
    "usando um exemplo de brinquedo em que as chaves e os valores são iguais.\n",
    "Como resultado,\n",
    "a forma da saída de atenção *multi-head*\n",
    "é (`batch_size`,` num_queries`, `num_hiddens`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (attention): DotProductAttention(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (W_q): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_k): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_v): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens, num_heads = 100, 5\n",
    "attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,\n",
    "                               num_hiddens, num_heads, 0.5)\n",
    "attention.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 100])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, num_queries, num_kvpairs, valid_lens = 2, 4, 6, torch.tensor([3, 2])\n",
    "X = torch.ones((batch_size, num_queries, num_hiddens))\n",
    "Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n",
    "attention(X, Y, Y, valid_lens).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 13
   },
   "source": [
    "## Resumo\n",
    "\n",
    "* A atenção *multi-head* combina o conhecimento do mesmo agrupamento de atenção por meio de diferentes subespaços de representação de consultas, chaves e valores.\n",
    "* Para calcular várias *heads* de atenção de *multi-heads* em paralelo, é necessária a manipulação adequada do tensor.\n",
    "\n",
    "\n",
    "\n",
    "## Exercícios\n",
    "\n",
    "1. Visualize o peso da atenção *multi-head* neste experimento.\n",
    "1. Suponha que temos um modelo treinado com base na atenção *multi-head* e queremos podar as *heads* menos importantes para aumentar a velocidade de previsão. Como podemos projetar experimentos para medir a importância de uma *head* de atenção?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 15,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1635)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "<!--stackedit_data:\n",
    "eyJoaXN0b3J5IjpbLTEzOTQ3NzkwNDMsLTEzOTI0MzE2MjQsMT\n",
    "QyMzkwNjgyMCwtOTc5MjA5NTQxLDE5NTU1NjIxMDVdfQ==\n",
    "-->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}