{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Mecanismos de Atenção\n",
    ":label:`chap_attention`\n",
    "\n",
    "\n",
    "O nervo óptico do sistema visual de um primata\n",
    "recebe entrada sensorial massiva,\n",
    "excedendo em muito o que o cérebro pode processar totalmente.\n",
    "Felizmente,\n",
    "nem todos os estímulos são criados iguais.\n",
    "Focalização e concentração de consciência\n",
    "permitiram que os primatas direcionassem a atenção\n",
    "para objetos de interesse,\n",
    "como presas e predadores,\n",
    "no ambiente visual complexo.\n",
    "A capacidade de prestar atenção a\n",
    "apenas uma pequena fração das informações\n",
    "tem significado evolutivo,\n",
    "permitindo seres humanos\n",
    "para viver e ter sucesso.\n",
    "\n",
    "Os cientistas têm estudado a atenção\n",
    "no campo da neurociência cognitiva\n",
    "desde o século XIX.\n",
    "Neste capítulo,\n",
    "começaremos revisando uma estrutura popular\n",
    "explicando como a atenção é implantada em uma cena visual.\n",
    "Inspirado pelas dicas de atenção neste quadro,\n",
    "nós iremos projetar modelos\n",
    "que alavancam tais dicas de atenção.\n",
    "Notavelmente, a regressão do kernel Nadaraya-Waston\n",
    "em 1964 é uma demonstração simples de aprendizado de máquina com *mecanismos de atenção*.\n",
    "\n",
    "\n",
    "A seguir, iremos apresentar as funções de atenção\n",
    "que têm sido amplamente usadas em\n",
    "o desenho de modelos de atenção em *deep learning*.\n",
    "Especificamente,\n",
    "vamos mostrar como usar essas funções\n",
    "para projetar a *atenção Bahdanau*,\n",
    "um modelo de atenção inovador em *deep learning*\n",
    "que pode se alinhar bidirecionalmente e é diferenciável.\n",
    "\n",
    "No fim,\n",
    "equipados com\n",
    "a mais recente\n",
    "*atenção de várias cabeças*\n",
    "e designs de *autoatenção*,\n",
    "iremos descrever a arquitetura do *transformador*\n",
    "baseado unicamente em mecanismos de atenção.\n",
    "Desde sua proposta em 2017,\n",
    "transformadores\n",
    "têm sido difundidos na modernidade\n",
    "aplicativos de *deep learning*,\n",
    "como em áreas de\n",
    "língua,\n",
    "visão, fala,\n",
    "e aprendizagem por reforço.\n",
    "\n",
    ":begin_tab:toc\n",
    " - [attention-cues](attention-cues.ipynb)\n",
    " - [nadaraya-waston](nadaraya-waston.ipynb)\n",
    " - [attention-scoring-functions](attention-scoring-functions.ipynb)\n",
    " - [bahdanau-attention](bahdanau-attention.ipynb)\n",
    " - [multihead-attention](multihead-attention.ipynb)\n",
    " - [self-attention-and-positional-encoding](self-attention-and-positional-encoding.ipynb)\n",
    " - [transformer](transformer.ipynb)\n",
    ":end_tab:\n",
    "\n",
    "<!--stackedit_data:\n",
    "eyJoaXN0b3J5IjpbLTE5MTIyOTkwNzAsLTM2MTUxNTI0NywtMT\n",
    "MxNDMzNDU4Ml19\n",
    "-->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}