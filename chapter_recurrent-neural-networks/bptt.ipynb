{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Retropropagação ao Longo do Tempo\n",
    ":label:`sec_bptt`\n",
    "\n",
    "\n",
    "Até agora, temos repetidamente aludido a coisas como\n",
    "*gradientes explosivos*,\n",
    "*gradientes de desaparecimento*,\n",
    "e a necessidade de\n",
    "*destacar o gradiente* para RNNs.\n",
    "Por exemplo, em :numref:`sec_rnn_scratch`\n",
    "invocamos a função `detach` na sequência.\n",
    "Nada disso foi realmente completamente\n",
    "explicado, no interesse de ser capaz de construir um modelo rapidamente e\n",
    "para ver como funciona.\n",
    "Nesta secção,\n",
    "vamos nos aprofundar um pouco mais\n",
    "nos detalhes de retropropagação para modelos de sequência e por que (e como) a matemática funciona.\n",
    "\n",
    "Encontramos alguns dos efeitos da explosão de gradiente quando primeiro\n",
    "RNNs implementados (:numref:`sec_rnn_scratch`).\n",
    "No\n",
    "especial,\n",
    "se você resolveu os exercícios,\n",
    "você poderia\n",
    "ter visto que o corte de gradiente é vital para garantir\n",
    "convergência.\n",
    "Para fornecer uma melhor compreensão deste problema, esta\n",
    "seção irá rever como os gradientes são calculados para modelos de sequência.\n",
    "Observe\n",
    "que não há nada conceitualmente novo em como funciona. Afinal, ainda estamos apenas aplicando a regra da cadeia para calcular gradientes. No entanto,\n",
    "vale a pena revisar a retropropagação (:numref:`sec_backprop`) novamente.\n",
    "\n",
    "\n",
    "Descrevemos propagações para frente e para trás\n",
    "e gráficos computacionais\n",
    "em MLPs em :numref:`sec_backprop`.\n",
    "A propagação direta em uma RNN é relativamente\n",
    "para a frente.\n",
    "*Retropropagação através do tempo* é, na verdade, uma aplicação específica de retropropagação\n",
    "em RNNs :cite:`Werbos.1990`.\n",
    "Isto\n",
    "exige que expandamos o\n",
    "gráfico computacional de uma RNN\n",
    "um passo de cada vez para\n",
    "obter as dependências\n",
    "entre variáveis ​​e parâmetros do modelo.\n",
    "Então,\n",
    "com base na regra da cadeia,\n",
    "aplicamos retropropagação para calcular e\n",
    "gradientes de loja.\n",
    "Uma vez que as sequências podem ser bastante longas, a dependência pode ser bastante longa.\n",
    "Por exemplo, para uma sequência de 1000 caracteres,\n",
    "o primeiro token pode ter uma influência significativa sobre o token na posição final.\n",
    "Isso não é realmente viável computacionalmente\n",
    "(leva muito tempo e requer muita memória) e requer mais de 1000 produtos de matriz antes de chegarmos a esse gradiente muito indescritível.\n",
    "Este é um processo repleto de incertezas computacionais e estatísticas.\n",
    "A seguir iremos elucidar o que acontece\n",
    "e como resolver isso na prática.\n",
    "\n",
    "## Análise de Gradientes em RNNs\n",
    ":label:`subsec_bptt_analysis`\n",
    "\n",
    "\n",
    "Começamos com um modelo simplificado de como funciona uma RNN.\n",
    "Este modelo ignora detalhes sobre as especificações do estado oculto e como ele é atualizado.\n",
    "A notação matemática aqui\n",
    "não distingue explicitamente\n",
    "escalares, vetores e matrizes como costumava fazer.\n",
    "Esses detalhes são irrelevantes para a análise\n",
    "e serviriam apenas para bagunçar a notação\n",
    "nesta subseção.\n",
    "\n",
    "Neste modelo simplificado,\n",
    "denotamos $h_t$ como o estado oculto,\n",
    "$x_t$ como a entrada e $o_t$ como a saída\n",
    "no passo de tempo $t$.\n",
    "Lembre-se de nossas discussões em\n",
    ":numref:`subsec_rnn_w_hidden_states`\n",
    "que a entrada e o estado oculto\n",
    "podem ser concatenados ao\n",
    "serem multiplicados por uma variável de peso na camada oculta.\n",
    "Assim, usamos $w_h$ e $w_o$ para\n",
    "indicar os pesos da camada oculta e da camada de saída, respectivamente.\n",
    "Como resultado, os estados ocultos e saídas em cada etapa de tempo podem ser explicados como\n",
    "\n",
    "$$\\begin{aligned}h_t &= f(x_t, h_{t-1}, w_h),\\\\o_t &= g(h_t, w_o),\\end{aligned}$$\n",
    ":eqlabel:`eq_bptt_ht_ot`\n",
    "\n",
    "onde $f$ e $g$ são transformações\n",
    "da camada oculta e da camada de saída, respectivamente.\n",
    "Portanto, temos uma cadeia de valores $\\{\\ldots, (x_{t-1}, h_{t-1}, o_{t-1}), (x_{t}, h_{t}, o_t), \\ldots\\}$ que dependem uns dos outros por meio de computação recorrente.\n",
    "A propagação direta é bastante direta.\n",
    "Tudo o que precisamos é percorrer as triplas $(x_t, h_t, o_t)$ um passo de tempo de cada vez.\n",
    "A discrepância entre a saída $o_t$ e o rótulo desejado $y_t$ é então avaliada por uma função objetivo\n",
    "em todas as etapas de tempo $T$\n",
    "como\n",
    "\n",
    "$$L(x_1, \\ldots, x_T, y_1, \\ldots, y_T, w_h, w_o) = \\frac{1}{T}\\sum_{t=1}^T l(y_t, o_t).$$\n",
    "\n",
    "\n",
    "\n",
    "Para retropropagação, as coisas são um pouco mais complicadas, especialmente quando calculamos os gradientes em relação aos parâmetros $w_h$ da função objetivo $L$. Para ser específico, pela regra da cadeia,\n",
    "\n",
    "$$\\begin{aligned}\\frac{\\partial L}{\\partial w_h}  & = \\frac{1}{T}\\sum_{t=1}^T \\frac{\\partial l(y_t, o_t)}{\\partial w_h}  \\\\& = \\frac{1}{T}\\sum_{t=1}^T \\frac{\\partial l(y_t, o_t)}{\\partial o_t} \\frac{\\partial g(h_t, w_o)}{\\partial h_t}  \\frac{\\partial h_t}{\\partial w_h}.\\end{aligned}$$\n",
    ":eqlabel:`eq_bptt_partial_L_wh`\n",
    "\n",
    "O primeiro e o segundo fatores do\n",
    "produto em :eqref:`eq_bptt_partial_L_wh`\n",
    "são fáceis de calcular.\n",
    "O terceiro fator $\\partial h_t/\\partial w_h$ é onde as coisas ficam complicadas, já que precisamos calcular recorrentemente o efeito do parâmetro $w_h$ em $h_t$.\n",
    "De acordo com o cálculo recorrente\n",
    "em :eqref:`eq_bptt_ht_ot`,\n",
    "$h_t$ depende de $h_{t-1}$ e $w_h$,\n",
    "onde cálculo de $h_{t-1}$\n",
    "também depende de $w_h$.\n",
    "Assim,\n",
    "usando a regra da cadeia temos\n",
    "\n",
    "$$\\frac{\\partial h_t}{\\partial w_h}= \\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial w_h} +\\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial w_h}.$$\n",
    ":eqlabel:`eq_bptt_partial_ht_wh_recur`\n",
    "\n",
    "\n",
    "Para derivar o gradiente acima, suponha que temos três sequências $\\{a_{t}\\},\\{b_{t}\\},\\{c_{t}\\}$ satisfatória\n",
    "$a_{0}=0$ and $a_{t}=b_{t}+c_{t}a_{t-1}$ for $t=1, 2,\\ldots$.\n",
    "Então, para $t\\geq 1$, é fácil mostrar\n",
    "\n",
    "$$a_{t}=b_{t}+\\sum_{i=1}^{t-1}\\left(\\prod_{j=i+1}^{t}c_{j}\\right)b_{i}.$$\n",
    ":eqlabel:`eq_bptt_at`\n",
    "\n",
    "Substituindo $$a_t$, $b_t$, e $c_t$\n",
    "de acordo com\n",
    "\n",
    "$$\\begin{aligned}a_t &= \\frac{\\partial h_t}{\\partial w_h},\\\\\n",
    "b_t &= \\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial w_h}, \\\\\n",
    "c_t &= \\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial h_{t-1}},\\end{aligned}$$\n",
    "\n",
    "o cálculo do gradiente em: eqref: `eq_bptt_partial_ht_wh_recur` satisfaz\n",
    "$a_{t}=b_{t}+c_{t}a_{t-1}$.\n",
    "Assim,\n",
    "por :eqref:`eq_bptt_at`,\n",
    "podemos remover o cálculo recorrente em :eqref:`eq_bptt_partial_ht_wh_recur`\n",
    "com\n",
    "\n",
    "$$\\frac{\\partial h_t}{\\partial w_h}=\\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial w_h}+\\sum_{i=1}^{t-1}\\left(\\prod_{j=i+1}^{t} \\frac{\\partial f(x_{j},h_{j-1},w_h)}{\\partial h_{j-1}} \\right) \\frac{\\partial f(x_{i},h_{i-1},w_h)}{\\partial w_h}.$$\n",
    ":eqlabel:`eq_bptt_partial_ht_wh_gen`\n",
    "\n",
    "Embora possamos usar a regra da cadeia para calcular \\partial h_t/\\partial w_h$ recursivamente, esta cadeia pode ficar muito longa sempre que $t$ for grande. Vamos discutir uma série de estratégias para lidar com esse problema.\n",
    "\n",
    "### Computação Completa\n",
    "\n",
    "Obviamente,\n",
    "podemos apenas calcular a soma total em\n",
    ":eqref:`eq_bptt_partial_ht_wh_gen`.\n",
    "Porém,\n",
    "isso é muito lento e os gradientes podem explodir,\n",
    "uma vez que mudanças sutis nas condições iniciais podem afetar muito o resultado.\n",
    "Ou seja, poderíamos ver coisas semelhantes ao efeito borboleta, em que mudanças mínimas nas condições iniciais levam a mudanças desproporcionais no resultado.\n",
    "Na verdade, isso é bastante indesejável em termos do modelo que queremos estimar.\n",
    "Afinal, estamos procurando estimadores robustos que generalizem bem. Portanto, essa estratégia quase nunca é usada na prática.\n",
    "\n",
    "### Truncamento de Etapas de Tempo\n",
    "\n",
    "Alternativamente,\n",
    "podemos truncar a soma em\n",
    ":eqref:`eq_bptt_partial_ht_wh_gen`\n",
    "após $\\tau$  passos.\n",
    "Isso é o que estivemos discutindo até agora,\n",
    "como quando separamos os gradientes em :numref:`sec_rnn_scratch`. \n",
    "Isso leva a uma *aproximação* do gradiente verdadeiro, simplesmente terminando a soma em\n",
    "$\\partial h_{t-\\tau}/\\partial w_h$. \n",
    "Na prática, isso funciona muito bem. É o que é comumente referido como retropropagação truncada ao longo do tempo :cite:`Jaeger.2002`.\n",
    "Uma das consequências disso é que o modelo se concentra principalmente na influência de curto prazo, e não nas consequências de longo prazo. Na verdade, isso é *desejável*, pois inclina a estimativa para modelos mais simples e estáveis.\n",
    "\n",
    "### Truncamento Randomizado ### \n",
    "\n",
    "Por último, podemos substituir $\\partial h_t/\\partial w_h$\n",
    "por uma variável aleatória que está correta na expectativa, mas trunca a sequência.\n",
    "Isso é conseguido usando uma sequência de $\\xi_t$\n",
    "com $0 \\leq \\pi_t \\leq 1$ predefinido,\n",
    "onde $P(\\xi_t = 0) = 1-\\pi_t$ e  $P(\\xi_t = \\pi_t^{-1}) = \\pi_t$,  portanto  $E[\\xi_t] = 1$.\n",
    "Usamos isso para substituir o gradiente\n",
    "$\\partial h_t/\\partial w_h$\n",
    "em :eqref:`eq_bptt_partial_ht_wh_recur`\n",
    "com\n",
    "\n",
    "$$z_t= \\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial w_h} +\\xi_t \\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial w_h}.$$\n",
    "\n",
    "\n",
    "Segue da definição de $\\xi_t$ that $E[z_t] = \\partial h_t/\\partial w_h$.\n",
    "Sempre que $\\xi_t = 0$ o cálculo recorrente\n",
    "termina nesse momento no passo $t$.\n",
    "Isso leva a uma soma ponderada de sequências de comprimentos variados, em que sequências longas são raras, mas apropriadamente sobrecarregadas.\n",
    "Esta ideia foi proposta por Tallec e Ollivier\n",
    ":cite:`Tallec.Ollivier.2017`.\n",
    "\n",
    "### Comparando Estratégias\n",
    "\n",
    "![Comparando estratégias para computar gradientes em RNNs. De cima para baixo: truncamento aleatório, truncamento regular e computação completa.](../img/truncated-bptt.svg)\n",
    ":label:`fig_truncated_bptt`\n",
    "\n",
    "\n",
    "\n",
    ":numref:`fig_truncated_bptt` ilustra as três estratégias ao analisar os primeiros caracteres do livro *The Time Machine* usando retropropagação através do tempo para RNNs:\n",
    "\n",
    "* A primeira linha é o truncamento aleatório que divide o texto em segmentos de comprimentos variados.\n",
    "* A segunda linha é o truncamento regular que divide o texto em subsequências do mesmo comprimento. Isso é o que temos feito em experimentos RNN.\n",
    "* A terceira linha é a retropropagação completa ao longo do tempo que leva a uma expressão computacionalmente inviável.\n",
    "\n",
    "\n",
    "Infelizmente, embora seja atraente em teoria, o truncamento aleatório não funciona muito melhor do que o truncamento regular, provavelmente devido a uma série de fatores.\n",
    "Primeiro, o efeito de uma observação após várias etapas de retropropagação no passado é suficiente para capturar dependências na prática.\n",
    "Segundo, o aumento da variância neutraliza o fato de que o gradiente é mais preciso com mais etapas.\n",
    "Terceiro, nós realmente *queremos* modelos que tenham apenas um curto intervalo de interações. Conseqüentemente, a retropropagação regularmente truncada ao longo do tempo tem um leve efeito de regularização que pode ser desejável.\n",
    "\n",
    "## Retropropagação ao Longo do Tempo em Detalhes\n",
    "\n",
    "Depois de discutir o princípio geral,\n",
    "vamos discutir a retropropagação ao longo do tempo em detalhes.\n",
    "Diferente da análise em\n",
    ":numref:`subsec_bptt_analysis`,\n",
    "na sequência\n",
    "vamos mostrar\n",
    "como calcular\n",
    "os gradientes da função objetivo\n",
    "com respeito a todos os parâmetros do modelo decomposto.\n",
    "Para manter as coisas simples, consideramos\n",
    "uma RNN sem parâmetros de polarização,\n",
    "cuja função de ativação na camada oculta\n",
    "usa o mapeamento de identidade ($\\phi(x)=x$).\n",
    "Para a etapa de tempo $t$,\n",
    "deixe a entrada de exemplo único e o rótulo ser\n",
    "$$\\mathbf{x}_t \\in \\mathbb{R}^d$ and $y_t$, respectivamente.\n",
    "O estado oculto $\\mathbf{h}_t \\in \\mathbb{R}^h$ \n",
    "e a saída $\\mathbf{o}_t \\in \\mathbb{R}^q$\n",
    "são computados como\n",
    "\n",
    "$$\\begin{aligned}\\mathbf{h}_t &= \\mathbf{W}_{hx} \\mathbf{x}_t + \\mathbf{W}_{hh} \\mathbf{h}_{t-1},\\\\\n",
    "\\mathbf{o}_t &= \\mathbf{W}_{qh} \\mathbf{h}_{t},\\end{aligned}$$\n",
    "\n",
    "onde $\\mathbf{W}_{hx} \\in \\mathbb{R}^{h \\times d}$,  $\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$, e\n",
    "$\\mathbf{W}_{qh} \\in \\mathbb{R}^{q \\times h}$\n",
    "são os parâmetros de peso.\n",
    "Denotar por $l(\\mathbf{o}_t, y_t)$\n",
    "a perda na etapa de tempo $t$.\n",
    "Nossa função objetivo,\n",
    "a perda em etapas de tempo de $T$\n",
    "desde o início da sequência\n",
    "é assim\n",
    "\n",
    "$$L = \\frac{1}{T} \\sum_{t=1}^T l(\\mathbf{o}_t, y_t).$$\n",
    "\n",
    "\n",
    "A fim de visualizar as dependências entre\n",
    "variáveis e parâmetros do modelo durante o cálculo\n",
    "do RNN,\n",
    "podemos desenhar um gráfico computacional para o modelo,\n",
    "como mostrado em :numref:`fig_rnn_bptt`.\n",
    "Por exemplo, o cálculo dos estados ocultos do passo de tempo 3, $\\mathbf{h}_3$, depende dos parâmetros do modelo $\\mathbf{W}_{hx}$ e $\\mathbf{W}_{hh}$,\n",
    "o estado oculto da última etapa de tempo $\\mathbf{h}_2$,\n",
    "e a entrada do intervalo de tempo atual $\\mathbf{x}_3$.\n",
    "\n",
    "![Gráfico computacional mostrando dependências para um modelo RNN com três intervalos de tempo. Caixas representam variáveis (não sombreadas) ou parâmetros (sombreados) e círculos representam operadores.](../img/rnn-bptt.svg)\n",
    ":label:`fig_rnn_bptt`\n",
    "\n",
    "Como acabamos de mencionar, os parâmetros do modelo em :numref:`fig_rnn_bptt` são $\\mathbf{W}_{hx}$, $\\mathbf{W}_{hh}$, e $\\mathbf{W}_{qh}$. \n",
    "Geralmente, treinar este modelo\n",
    "requer cálculo de gradiente em relação a esses parâmetros\n",
    "$\\partial L/\\partial \\mathbf{W}_{hx}$, $\\partial L/\\partial \\mathbf{W}_{hh}$, e $\\partial L/\\partial \\mathbf{W}_{qh}$.\n",
    "De acordo com as dependências em :numref:`fig_rnn_bptt`,\n",
    "nós podemos atravessar\n",
    "na direção oposta das setas\n",
    "para calcular e armazenar os gradientes por sua vez.\n",
    "Para expressar de forma flexível a multiplicação\n",
    "de matrizes, vetores e escalares de diferentes formas\n",
    "na regra da cadeia,\n",
    "nós continuamos a usar\n",
    "o operador $\\text{prod}$ conforme descrito em\n",
    ":numref:`sec_backprop`.\n",
    "\n",
    "Em primeiro lugar,\n",
    "diferenciando a função objetivo\n",
    "com relação à saída do modelo\n",
    "a qualquer momento, etapa $t$\n",
    "é bastante simples:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{o}_t} =  \\frac{\\partial l (\\mathbf{o}_t, y_t)}{T \\cdot \\partial \\mathbf{o}_t} \\in \\mathbb{R}^q.$$\n",
    ":eqlabel:`eq_bptt_partial_L_ot`\n",
    "\n",
    "Agora, podemos calcular o gradiente da função objetivo\n",
    "em relação ao parâmetro $\\mathbf{W}_{qh}$\n",
    "na camada de saída:\n",
    "$\\partial L/\\partial \\mathbf{W}_{qh} \\in \\mathbb{R}^{q \\times h}$. Com base em :numref:`fig_rnn_bptt`, \n",
    "a função objetivo\n",
    "$L$ depende de $\\mathbf{W}_{qh}$ via $\\mathbf{o}_1, \\ldots, \\mathbf{o}_T$. Usar a regra da cadeia produz\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}_{qh}}\n",
    "= \\sum_{t=1}^T \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{o}_t}, \\frac{\\partial \\mathbf{o}_t}{\\partial \\mathbf{W}_{qh}}\\right)\n",
    "= \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\mathbf{o}_t} \\mathbf{h}_t^\\top,\n",
    "$$\n",
    "\n",
    "\n",
    "onde $\\partial L/\\partial \\mathbf{o}_t$\n",
    "é fornecido por :eqref:`eq_bptt_partial_L_ot`.\n",
    "\n",
    "A seguir, conforme mostrado em :numref:`fig_rnn_bptt`,\n",
    "no tempo final, passo $T$\n",
    "a função objetivo\n",
    "$L$ depende do estado oculto $\\mathbf{h}_T$ apenas via $\\mathbf{o}_T$.\n",
    "Portanto, podemos facilmente encontrar\n",
    "o gradiente\n",
    "$\\partial L/\\partial \\mathbf{h}_T \\in \\mathbb{R}^h$\n",
    "usando a regra da cadeia:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{h}_T} = \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{o}_T}, \\frac{\\partial \\mathbf{o}_T}{\\partial \\mathbf{h}_T} \\right) = \\mathbf{W}_{qh}^\\top \\frac{\\partial L}{\\partial \\mathbf{o}_T}.$$\n",
    ":eqlabel:`eq_bptt_partial_L_hT_final_step`\n",
    "\n",
    "Fica mais complicado para qualquer passo de tempo $t < T$,\n",
    "onde a função objetivo $L$ depende de $\\mathbf{h}_t$ via $\\mathbf{h}_{t+1}$ e $\\mathbf{o}_t$.\n",
    "De acordo com a regra da cadeia,\n",
    "o gradiente do estado oculto\n",
    "$\\partial L/\\partial \\mathbf{h}_t \\in \\mathbb{R}^h$\n",
    "a qualquer momento, o passo $t<T$ pode ser calculado recorrentemente como:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{h}_t} = \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{h}_{t+1}}, \\frac{\\partial \\mathbf{h}_{t+1}}{\\partial \\mathbf{h}_t} \\right) + \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{o}_t}, \\frac{\\partial \\mathbf{o}_t}{\\partial \\mathbf{h}_t} \\right) = \\mathbf{W}_{hh}^\\top \\frac{\\partial L}{\\partial \\mathbf{h}_{t+1}} + \\mathbf{W}_{qh}^\\top \\frac{\\partial L}{\\partial \\mathbf{o}_t}.$$\n",
    ":eqlabel:`eq_bptt_partial_L_ht_recur`\n",
    "\n",
    "Para análise,\n",
    "expandindo a computação recorrente\n",
    "para qualquer etapa de tempo $1 \\leq t \\leq T$\n",
    "dá\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{h}_t}= \\sum_{i=t}^T {\\left(\\mathbf{W}_{hh}^\\top\\right)}^{T-i} \\mathbf{W}_{qh}^\\top \\frac{\\partial L}{\\partial \\mathbf{o}_{T+t-i}}.$$\n",
    ":eqlabel:`eq_bptt_partial_L_ht`\n",
    "\n",
    "\n",
    "Podemos ver em :eqref:`eq_bptt_partial_L_ht` que\n",
    "este exemplo linear simples já\n",
    "exibe alguns problemas-chave de modelos de sequência longa: envolve potências potencialmente muito grandes de $\\mathbf{W}_{hh}^\\top$.\n",
    "Nele, autovalores menores que 1 desaparecem\n",
    "e os autovalores maiores que 1 divergem.\n",
    "Isso é numericamente instável,\n",
    "que se manifesta na forma de desaparecimento\n",
    "e gradientes explosivos.\n",
    "Uma maneira de resolver isso é truncar as etapas de tempo\n",
    "em um tamanho computacionalmente conveniente conforme discutido em :numref:`subsec_bptt_analysis`. \n",
    "Na prática, esse truncamento é efetuado destacando-se o gradiente após um determinado número de etapas de tempo.\n",
    "Mais tarde\n",
    "veremos como modelos de sequência mais sofisticados, como a memória de curto prazo longa, podem aliviar ainda mais isso.\n",
    "\n",
    "Finalmente,\n",
    ":numref:`fig_rnn_bptt` mostra que\n",
    "a função objetivo\n",
    "$L$ depende dos parâmetros do modelo\n",
    "$\\mathbf{W}_{hx}$ e $\\mathbf{W}_{hh}$\n",
    "na camada oculta\n",
    "via estados ocultos\n",
    "$\\mathbf{h}_1, \\ldots, \\mathbf{h}_T$.\n",
    "Para calcular gradientes\n",
    "com respeito a tais parâmetros\n",
    "$\\partial L / \\partial \\mathbf{W}_{hx} \\in \\mathbb{R}^{h \\times d}$ e $\\partial L / \\partial \\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$,\n",
    "aplicamos a regra da cadeia que dá\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}_{hx}}\n",
    "&= \\sum_{t=1}^T \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{h}_t}, \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{W}_{hx}}\\right)\n",
    "= \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\mathbf{h}_t} \\mathbf{x}_t^\\top,\\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}_{hh}}\n",
    "&= \\sum_{t=1}^T \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{h}_t}, \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{W}_{hh}}\\right)\n",
    "= \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\mathbf{h}_t} \\mathbf{h}_{t-1}^\\top,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "Onde\n",
    "$\\partial L/\\partial \\mathbf{h}_t$\n",
    "que é calculado recorrentemente por\n",
    ":eqref:`eq_bptt_partial_L_hT_final_step`\n",
    "e\n",
    ":eqref:`eq_bptt_partial_L_ht_recur`\n",
    "é a quantidade chave\n",
    "que afeta a estabilidade numérica.\n",
    "\n",
    "\n",
    "\n",
    "Como a retropropagação através do tempo\n",
    "é a aplicação de retropropagação em RNNs,\n",
    "como explicamos em :numref:`sec_backprop`,\n",
    "o treinamento de RNNs\n",
    "alterna a propagação direta com\n",
    "retropropagação através do tempo.\n",
    "Além do mais,\n",
    "retropropagação através do tempo\n",
    "calcula e armazena os gradientes acima\n",
    "por sua vez.\n",
    "Especificamente,\n",
    "valores intermediários armazenados\n",
    "são reutilizados\n",
    "para evitar cálculos duplicados,\n",
    "como armazenar\n",
    "$\\partial L/\\partial \\mathbf{h}_t$\n",
    "para ser usado no cálculo de  $\\partial L / \\partial \\mathbf{W}_{hx}$ e $\\partial L / \\partial \\mathbf{W}_{hh}$.\n",
    "\n",
    "\n",
    "## Resumo\n",
    "\n",
    "* A retropropagação através do tempo é meramente uma aplicação da retropropagação para sequenciar modelos com um estado oculto.\n",
    "* O truncamento é necessário para conveniência computacional e estabilidade numérica, como truncamento regular e truncamento aleatório.\n",
    "* Altos poderes de matrizes podem levar a autovalores divergentes ou desaparecendo. Isso se manifesta na forma de gradientes explodindo ou desaparecendo.\n",
    "* Para computação eficiente, os valores intermediários são armazenados em cache durante a retropropagação ao longo do tempo.\n",
    "\n",
    "\n",
    "\n",
    "## Exercícios\n",
    "\n",
    "1. Suponha que temos uma matriz simétrica $\\mathbf{M} \\in \\mathbb{R}^{n \\times n}$ with eigenvalues $\\lambda_i$ cujos autovetores correspondentes são $\\mathbf{v}_i$ ($i = 1, \\ldots, n$). Sem perda de generalidade, assuma que eles estão ordenados na ordem $|\\lambda_i| \\geq |\\lambda_{i+1}|$. \n",
    "    1. Mostre que $\\mathbf{M}^k$ tem autovalores $\\lambda_i^k$.\n",
    "    1. Prove que para um vetor aleatório $\\mathbf{x} \\in \\mathbb{R}^n$, com alta probabilidade $\\mathbf{M}^k \\mathbf{x}$  estará muito alinhado com o autovetor $\\mathbf{v}_1$ de $\\mathbf{M}$. Formalize esta declaração.\n",
    "    1. O que o resultado acima significa para gradientes em RNNs?\n",
    "1. Além do recorte de gradiente, você consegue pensar em outros métodos para lidar com a explosão de gradiente em redes neurais recorrentes?\n",
    "\n",
    "[Discussions](https://discuss.d2l.ai/t/334)\n",
    "<!--stackedit_data:\n",
    "eyJoaXN0b3J5IjpbODMxNDY5NTkwLDU4MDc2MjQ2NywtMTQzNT\n",
    "gwMTMzMCwtMTYzNTEyOTY0MCwtODc1MDA3ODk0LDE5MDU0NzE5\n",
    "NTMsNTAwNDY0NTMyLC0xMTAyNzY5NDA0XX0=\n",
    "-->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}