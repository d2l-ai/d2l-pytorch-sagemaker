{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing (updating) the following libraries for your Sagemaker\n",
    "instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install .. # installing d2l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Redes Neurais Recorrentes (RNNs)\n",
    ":label:`sec_rnn`\n",
    "\n",
    "\n",
    "Em :numref:`sec_language_model` introduzimos modelos de $n$-gramas, onde a probabilidade condicional da palavra $x_t$ no passo de tempo $t$ depende apenas das $n-1$ palavras anteriores.\n",
    "Se quisermos incorporar o possível efeito de palavras anteriores ao passo de tempo $t-(n-1)$ em $x_t$,\n",
    "precisamos aumentar $n$.\n",
    "No entanto, o número de parâmetros do modelo também aumentaria exponencialmente com ele, pois precisamos armazenar $|\\mathcal{V}|^n$  números para um conjunto de vocabulário $\\mathcal{V}$.\n",
    "Portanto, em vez de modelar $P(x_t \\mid x_{t-1}, \\ldots, x_{t-n+1})$, é preferível usar um modelo de variável latente:\n",
    "\n",
    "$$P(x_t \\mid x_{t-1}, \\ldots, x_1) \\approx P(x_t \\mid h_{t-1}),$$\n",
    "\n",
    "onde $h_{t-1}$ é um *estado oculto* (também conhecido como uma variável oculta) que armazena as informações da sequência até o passo de tempo $t-1$.\n",
    "Em geral,\n",
    "o estado oculto em qualquer etapa $t$ pode ser calculado com base na entrada atual $x_ {t}$ e no estado oculto anterior $h_ {t-1}$:\n",
    "\n",
    "$$h_t = f(x_{t}, h_{t-1}).$$\n",
    ":eqlabel:`eq_ht_xt`\n",
    "\n",
    "\n",
    "Para uma função suficientemente poderosa $f$ em :eqref:`eq_ht_xt`, o modelo de variável latente não é uma aproximação. Afinal, $h_t$ pode simplesmente armazenar todos os dados que observou até agora.\n",
    "No entanto, isso pode tornar a computação e o armazenamento caros.\n",
    "\n",
    "Lembre-se de que discutimos camadas ocultas com unidades ocultas em :numref:`chap_perceptrons`.\n",
    "É digno de nota que\n",
    "camadas ocultas e estados ocultos referem-se a dois conceitos muito diferentes.\n",
    "Camadas ocultas são, conforme explicado, camadas que ficam ocultas da visualização no caminho da entrada à saída.\n",
    "Estados ocultos são tecnicamente falando *entradas* para tudo o que fazemos em uma determinada etapa,\n",
    "e elas só podem ser calculadas observando os dados em etapas de tempo anteriores.\n",
    "\n",
    "*Redes neurais recorrentes* (RNNs) são redes neurais com estados ocultos. Antes de introduzir o modelo RNN, primeiro revisitamos o modelo MLP introduzido em :numref:`sec_mlp`.\n",
    "\n",
    "## Redes Neurais sem Estados Ocultos\n",
    "\n",
    "Vamos dar uma olhada em um MLP com uma única camada oculta.\n",
    "Deixe a função de ativação da camada oculta ser $\\phi$.\n",
    "Dado um minibatch de exemplos $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ com tamanho de lote $n$ e $d$ entradas, a saída da camada oculta $\\mathbf{H} \\in \\mathbb{R}^{n \\times h}$ é calculada como\n",
    "\n",
    "$$\\mathbf{H} = \\phi(\\mathbf{X} \\mathbf{W}_{xh} + \\mathbf{b}_h).$$\n",
    ":eqlabel:`rnn_h_without_state`\n",
    "\n",
    "Em :eqref:`rnn_h_without_state`, temos o parâmetro de peso $\\mathbf{W}_{xh} \\in \\mathbb{R}^{d \\times h}$, o parâmetro de polarização $\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$, e o número de unidades ocultas $h$, para a camada oculta.\n",
    "Assim, a transmissão (ver :numref:`subsec_broadcasting`) é aplicada durante a soma.\n",
    "Em seguida, a variável oculta $\\mathbf{H}$ é usada como entrada da camada de saída. A camada de saída é fornecida por\n",
    "\n",
    "$$\\mathbf{O} = \\mathbf{H} \\mathbf{W}_{hq} + \\mathbf{b}_q,$$\n",
    "\n",
    "\n",
    "onde $\\mathbf{O} \\in \\mathbb{R}^{n \\times q}$ é a variável de saída, $\\mathbf{W}_{hq} \\in \\mathbb{R}^{h \\times q}$ é o parâmetro de peso, e $\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$ é o parâmetro de polarização da camada de saída. Se for um problema de classificação, podemos usar $\\text{softmax}(\\mathbf{O})$ para calcular a distribuição de probabilidade das categorias de saída.\n",
    "\n",
    "Isso é inteiramente análogo ao problema de regressão que resolvemos anteriormente em :numref:`sec_sequence`, portanto omitimos detalhes.\n",
    "Basta dizer que podemos escolher pares de rótulo de recurso aleatoriamente e aprender os parâmetros de nossa rede por meio de diferenciação automática e gradiente descendente estocástico.\n",
    "\n",
    "## Redes Neurais Recorrentes com Estados Ocultos\n",
    ":label:`subsec_rnn_w_hidden_states`\n",
    "\n",
    "\n",
    "As coisas são totalmente diferentes quando temos estados ocultos. Vejamos a estrutura com mais detalhes.\n",
    "\n",
    "Suponha que temos\n",
    "um minibatch de entradas\n",
    "$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$\n",
    "no passo de tempo $t$.\n",
    "Em outras palavras,\n",
    "para um minibatch de exemplos de sequência $n$,\n",
    "cada linha de $\\mathbf{X}_t$ corresponde a um exemplo no passo de tempo $t$ da sequência.\n",
    "Em seguida,\n",
    "denote por $\\mathbf{H}_t  \\in \\mathbb{R}^{n \\times h}$ a variável oculta do passo de tempo $t$.\n",
    "Ao contrário do MLP, aqui salvamos a variável oculta $\\mathbf{H}_{t-1}$ da etapa de tempo anterior e introduzimos um novo parâmetro de peso $\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$ para descrever como usar a variável oculta da etapa de tempo anterior na etapa de tempo atual. Especificamente, o cálculo da variável oculta da etapa de tempo atual é determinado pela entrada da etapa de tempo atual junto com a variável oculta da etapa de tempo anterior:\n",
    "\n",
    "$$\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}  + \\mathbf{b}_h).$$\n",
    ":eqlabel:`rnn_h_with_state`\n",
    "\n",
    "\n",
    "Comparado com :eqref:`rnn_h_without_state`,  :eqref:`rnn_h_with_state` adiciona mais um termo $\\mathbf{H}_{t-1} \\mathbf{W}_{hh}$ e assim\n",
    "instancia :eqref:`eq_ht_xt`.\n",
    "A partir da relação entre as variáveis ​​ocultas $\\mathbf{H}_t$  e $\\mathbf{H}_{t-1}$ de etapas de tempo adjacentes,\n",
    "sabemos que essas variáveis ​​capturaram e retiveram as informações históricas da sequência até sua etapa de tempo atual, assim como o estado ou a memória da etapa de tempo atual da rede neural. Portanto, essa variável oculta é chamada de *estado oculto*.\n",
    "Visto que o estado oculto usa a mesma definição da etapa de tempo anterior na etapa de tempo atual, o cálculo de :eqref:`rnn_h_with_state` é *recorrente*. Consequentemente, redes neurais com estados ocultos\n",
    "com base em cálculos recorrentes são nomeados\n",
    "*redes neurais recorrentes*.\n",
    "Camadas que fazem\n",
    "o cálculo de :eqref:`rnn_h_with_state`\n",
    "em RNNs\n",
    "são chamadas de *camadas recorrentes*.\n",
    "\n",
    "\n",
    "Existem muitas maneiras diferentes de construir RNNs.\n",
    "RNNs com um estado oculto definido por :eqref:`rnn_h_with_state` são muito comuns.\n",
    "Para a etapa de tempo $t$,\n",
    "a saída da camada de saída é semelhante à computação no MLP:\n",
    "\n",
    "$$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q.$$\n",
    "\n",
    "Parâmetros do RNN\n",
    "incluem os pesos $\\mathbf{W}_{xh} \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$,\n",
    "e o *bias* $\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$\n",
    "da camada oculta,\n",
    "junto com os pesos $\\mathbf{W}_{hq} \\in \\mathbb{R}^{h \\times q}$\n",
    "e o *bias*  $\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$\n",
    "da camada de saída.\n",
    "Vale a pena mencionar que\n",
    "mesmo em diferentes etapas de tempo,\n",
    "Os RNNs sempre usam esses parâmetros do modelo.\n",
    "Portanto, o custo de parametrização de um RNN\n",
    "não cresce à medida que o número de etapas de tempo aumenta.\n",
    "\n",
    ":numref:`fig_rnn` ilustra a lógica computacional de uma RNN em três etapas de tempo adjacentes.\n",
    "A qualquer momento, passo $t$,\n",
    "o cálculo do estado oculto pode ser tratado como:\n",
    "i) concatenar a entrada $\\mathbf{X}_t$ na etapa de tempo atual $t$ e o estado oculto $\\mathbf{H}_{t-1}$ na etapa de tempo anterior $t-1$;\n",
    "ii) alimentar o resultado da concatenação em uma camada totalmente conectada com a função de ativação $\\phi$.\n",
    "A saída dessa camada totalmente conectada é o estado oculto $\\mathbf{H}_t$ do intervalo de tempo atual $t$.\n",
    "Nesse caso,\n",
    "os parâmetros do modelo são a concatenação de $\\mathbf{W}_{xh}$ e $\\mathbf{W}_{hh}$, e um *bias* de $\\mathbf{b}_h$, tudo de :eqref:`rnn_h_with_state`.\n",
    "O estado oculto do passo de tempo atual $t$, $\\mathbf{H}_t$, participará do cálculo do estado oculto $\\mathbf{H}_{t+1}$ do próximo passo de tempo $t+1$.\n",
    "Além disso, $\\mathbf{H}_t$ também será\n",
    "alimentado na camada de saída totalmente conectada\n",
    "para calcular a saída\n",
    "$\\mathbf{O}_t$ do passo de tempo atual $t$.\n",
    "\n",
    "![Uma RNN com um estado oculto.](../img/rnn.svg)\n",
    ":label:`fig_rnn`\n",
    "\n",
    "Acabamos de mencionar que o cálculo de $\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}$ para o estado oculto é equivalente a\n",
    "multiplicação de matriz de\n",
    "concatenação de $\\mathbf{X}_t$ and $\\mathbf{H}_{t-1}$\n",
    "e\n",
    "concatenação de $\\mathbf{W}_{xh}$ and $\\mathbf{W}_{hh}$.\n",
    "Embora isso possa ser comprovado pela matemática,\n",
    "a seguir, apenas usamos um trecho de código simples para mostrar isso.\n",
    "Começando por,\n",
    "definir as matrizes `X`,` W_xh`, `H` e` W_hh`, cujas formas são (3, 1), (1, 4), (3, 4) e (4, 4), respectivamente.\n",
    "Multiplicando `X` por` W_xh`, e `H` por` W_hh`, respectivamente e, em seguida, adicionando essas duas multiplicações,\n",
    "obtemos uma matriz de forma (3, 4).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 4,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9656, -0.2889,  1.2013,  0.3412],\n",
       "        [ 2.0865, -2.5401,  1.2020,  3.0863],\n",
       "        [-1.5326,  0.3890, -4.6161, -0.2931]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, W_xh = torch.normal(0, 1, (3, 1)), torch.normal(0, 1, (1, 4))\n",
    "H, W_hh = torch.normal(0, 1, (3, 4)), torch.normal(0, 1, (4, 4))\n",
    "torch.matmul(X, W_xh) + torch.matmul(H, W_hh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "Agora vamos concatenar as matrizes `X` e` H`\n",
    "ao longo das colunas (eixo 1),\n",
    "e as matrizes\n",
    "`W_xh` e` W_hh` ao longo das linhas (eixo 0).\n",
    "Essas duas concatenações\n",
    "resulta em\n",
    "matrizes de forma (3, 5)\n",
    "e da forma (5, 4), respectivamente.\n",
    "Multiplicando essas duas matrizes concatenadas,\n",
    "obtemos a mesma matriz de saída de forma (3, 4)\n",
    "como acima.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9656, -0.2889,  1.2013,  0.3412],\n",
       "        [ 2.0865, -2.5401,  1.2020,  3.0863],\n",
       "        [-1.5326,  0.3890, -4.6161, -0.2931]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(torch.cat((X, H), 1), torch.cat((W_xh, W_hh), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "## Modelos de Linguagem em Nível de Caracteres Baseados em RNN\n",
    "\n",
    "Lembre-se que para a modelagem de linguagem em :numref:`sec_language_model`,\n",
    "pretendemos prever o próximo token com base em\n",
    "os tokens atuais e passados,\n",
    "assim, mudamos a sequência original em um token\n",
    "como os rótulos.\n",
    "Bengio et al. propuseram primeiro\n",
    "usar uma rede neural para modelagem de linguagem :cite:`Bengio.Ducharme.Vincent.ea.2003`.\n",
    "A seguir, ilustramos como os RNNs podem ser usadas para construir um modelo de linguagem.\n",
    "Deixe o tamanho do minibatch ser um e a sequência do texto ser \"máquina\".\n",
    "Para simplificar o treinamento nas seções subsequentes,\n",
    "nós tokenizamos o texto em caracteres em vez de palavras\n",
    "e considere um *modelo de linguagem em nível de caractere*.\n",
    " :numref:`fig_rnn_train` demonstra como prever o próximo caractere com base nos caracteres atuais e anteriores através de um RNN para modelagem de linguagem em nível de caractere.\n",
    "\n",
    "![Um modelo de linguagem de nível de caractere baseado no RNN. As sequências de entrada e rótulo são \"machin\" e \"achine\", respectivamente.](../img/rnn-train.svg)\n",
    ":label:`fig_rnn_train`\n",
    "\n",
    "\n",
    "Durante o processo de treinamento,\n",
    "executamos uma operação softmax na saída da camada de saída para cada etapa de tempo e, em seguida, usamos a perda de entropia cruzada para calcular o erro entre a saída do modelo e o rótulo.\n",
    "Devido ao cálculo recorrente do estado oculto na camada oculta, a saída da etapa de tempo 3 em :numref:`fig_rnn_train`,\n",
    "$\\mathbf{O}_3$, é determinada pela sequência de texto \"m\", \"a\" e \"c\". Como o próximo caractere da sequência nos dados de treinamento é \"h\", a perda de tempo da etapa 3 dependerá da distribuição de probabilidade do próximo caractere gerado com base na sequência de características \"m\", \"a\", \"c\" e o rótulo \"h\" desta etapa de tempo.\n",
    "\n",
    "Na prática, cada token é representado por um vetor $d$-dimensional e usamos um tamanho de batch $n>1$. Portanto, a entrada $ \\ mathbf X_t $ no passo de tempo $ t $ será uma matriz $\\mathbf X_t$, que é idêntica ao que discutimos em :numref:`subsec_rnn_w_hidden_states`.\n",
    "\n",
    "\n",
    "## Perplexidade\n",
    ":label:`subsec_perplexity`\n",
    "\n",
    "\n",
    "Por último, vamos discutir sobre como medir a qualidade do modelo de linguagem, que será usado para avaliar nossos modelos baseados em RNN nas seções subsequentes.\n",
    "Uma maneira é verificar o quão surpreendente é o texto.\n",
    "Um bom modelo de linguagem é capaz de prever com\n",
    "tokens de alta precisão que veremos a seguir.\n",
    "Considere as seguintes continuações da frase \"Está chovendo\", conforme proposto por diferentes modelos de linguagem:\n",
    "\n",
    "1. \"Está chovendo lá fora\"\n",
    "1. \"Está chovendo bananeira\"\n",
    "1. \"Está chovendo piouw; kcj pwepoiut\"\n",
    "\n",
    "Em termos de qualidade, o exemplo 1 é claramente o melhor. As palavras são sensatas e logicamente coerentes.\n",
    "Embora possa não refletir com muita precisão qual palavra segue semanticamente (\"em São Francisco\" e \"no inverno\" seriam extensões perfeitamente razoáveis), o modelo é capaz de capturar qual tipo de palavra se segue.\n",
    "O exemplo 2 é consideravelmente pior ao produzir uma extensão sem sentido. No entanto, pelo menos o modelo aprendeu como soletrar palavras e algum grau de correlação entre as palavras. Por último, o exemplo 3 indica um modelo mal treinado que não ajusta os dados adequadamente.\n",
    "\n",
    "\n",
    "Podemos medir a qualidade do modelo calculando a probabilidade da sequência.\n",
    "Infelizmente, esse é um número difícil de entender e difícil de comparar.\n",
    "Afinal, as sequências mais curtas têm muito mais probabilidade de ocorrer do que as mais longas,\n",
    "portanto, avaliando o modelo na magnum opus de Tolstoy\n",
    "*Guerra e paz* produzirá inevitavelmente uma probabilidade muito menor do que, digamos, na novela de Saint-Exupéry *O Pequeno Príncipe*. O que falta equivale a uma média.\n",
    "\n",
    "A teoria da informação é útil aqui.\n",
    "Definimos entropia, surpresa e entropia cruzada\n",
    "quando introduzimos a regressão softmax\n",
    "(:numref:`subsec_info_theory_basics`)\n",
    "e mais sobre a teoria da informação é discutido no [apêndice online sobre teoria da informação](https://d2l.ai/chapter_apencha-mathematics-for-deep-learning/information-theory.html).\n",
    "Se quisermos compactar o texto, podemos perguntar sobre\n",
    "prever o próximo token dado o conjunto atual de tokens.\n",
    "Um modelo de linguagem melhor deve nos permitir prever o próximo token com mais precisão.\n",
    "Assim, deve permitir-nos gastar menos bits na compressão da sequência.\n",
    "Então, podemos medi-lo pela perda de entropia cruzada média\n",
    "sobre todos os $n$ tokens de uma sequência:\n",
    "\n",
    "$$\\frac{1}{n} \\sum_{t=1}^n -\\log P(x_t \\mid x_{t-1}, \\ldots, x_1),$$\n",
    ":eqlabel:`eq_avg_ce_for_lm`\n",
    "\n",
    "onde $P$ é dado por um modelo de linguagem e $x_t$ é o token real observado no passo de tempo $t$ da sequência.\n",
    "Isso torna o desempenho em documentos de comprimentos diferentes comparáveis. Por razões históricas, os cientistas do processamento de linguagem natural preferem usar uma quantidade chamada *perplexidade*. Em poucas palavras, é a exponencial de :eqref:`eq_avg_ce_for_lm`:\n",
    "\n",
    "$$\\exp\\left(-\\frac{1}{n} \\sum_{t=1}^n \\log P(x_t \\mid x_{t-1}, \\ldots, x_1)\\right).$$\n",
    "\n",
    "\n",
    "A perplexidade pode ser melhor entendida como a média harmônica do número de escolhas reais que temos ao decidir qual ficha escolher a seguir. Vejamos alguns casos:\n",
    "\n",
    "* No melhor cenário, o modelo sempre estima perfeitamente a probabilidade do token de rótulo como 1. Nesse caso, a perplexidade do modelo é 1.\n",
    "* No pior cenário, o modelo sempre prevê a probabilidade do token de rótulo como 0. Nessa situação, a perplexidade é infinita positiva.\n",
    "* Na linha de base, o modelo prevê uma distribuição uniforme de todos os tokens disponíveis do vocabulário. Nesse caso, a perplexidade é igual ao número de tokens exclusivos do vocabulário. Na verdade, se armazenássemos a sequência sem nenhuma compressão, seria o melhor que poderíamos fazer para codificá-la. Conseqüentemente, isso fornece um limite superior não trivial que qualquer modelo útil deve superar.\n",
    "\n",
    "Nas seções a seguir, implementaremos RNNs\n",
    "para modelos de linguagem em nível de personagem e usaremos perplexidade\n",
    "para avaliar tais modelos.\n",
    "\n",
    "\n",
    "## Resumo\n",
    "\n",
    "* Uma rede neural que usa computação recorrente para estados ocultos é chamada de rede neural recorrente (RNN).\n",
    "* O estado oculto de uma RNN pode capturar informações históricas da sequência até o intervalo de tempo atual.\n",
    "* O número de parâmetros do modelo RNN não aumenta com o aumento do número de etapas de tempo.\n",
    "* Podemos criar modelos de linguagem em nível de caractere usando um RNN.\n",
    "* Podemos usar a perplexidade para avaliar a qualidade dos modelos de linguagem.\n",
    "\n",
    "## Exercícios\n",
    "\n",
    "1. Se usarmos uma RNN para prever o próximo caractere em uma sequência de texto, qual é a dimensão necessária para qualquer saída?\n",
    "1. Por que as RNNs podem expressar a probabilidade condicional de um token em algum intervalo de tempo com base em todos os tokens anteriores na sequência de texto?\n",
    "1. O que acontece com o gradiente se você retropropaga através de uma longa sequência?\n",
    "1. Quais são alguns dos problemas associados ao modelo de linguagem descrito nesta seção?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1050)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "<!--stackedit_data:\n",
    "eyJoaXN0b3J5IjpbNTM1ODI1MzU3LDE3ODA4MTczMjUsMjAwMz\n",
    "Y0MTY1Niw2ODIxMDI5MDAsLTgxMDY0OTA2MV19\n",
    "-->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}