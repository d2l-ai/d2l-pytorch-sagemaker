{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00126fa9",
   "metadata": {},
   "source": [
    "Installing (updating) the following libraries for your Sagemaker\n",
    "instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe8a184",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install .. # installing d2l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfdd208",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Bidirectional Recurrent Neural Networks\n",
    ":label:`sec_bi_rnn`\n",
    "\n",
    "So far, our working example of a sequence learning task has been language modeling,\n",
    "where we aim to predict the next token given all previous tokens in a sequence. \n",
    "In this scenario, we wish only to condition upon the leftward context,\n",
    "and thus the unidirectional chaining of a standard RNN seems appropriate. \n",
    "However, there are many other sequence learning tasks contexts \n",
    "where it's perfectly fine to condition the prediction at every time step\n",
    "on both the leftward and the rightward context. \n",
    "Consider, for example, part of speech detection. \n",
    "Why shouldn't we take the context in both directions into account\n",
    "when assessing the part of speech associated with a given word?\n",
    "\n",
    "Another common task---often useful as a pretraining exercise\n",
    "prior to fine-tuning a model on an actual task of interest---is\n",
    "to mask out random tokens in a text document and then to train \n",
    "a sequence model to predict the values of the missing tokens.\n",
    "Note that depending on what comes after the blank,\n",
    "the likely value of the missing token changes dramatically:\n",
    "\n",
    "* I am `___`.\n",
    "* I am `___` hungry.\n",
    "* I am `___` hungry, and I can eat half a pig.\n",
    "\n",
    "In the first sentence \"happy\" seems to be a likely candidate.\n",
    "The words \"not\" and \"very\" seem plausible in the second sentence, \n",
    "but \"not\" seems incompatible with the third sentences. \n",
    "\n",
    "\n",
    "Fortunately, a simple technique transforms any unidirectional RNN \n",
    "into a bidrectional RNN :cite:`Schuster.Paliwal.1997`.\n",
    "We simply implement two unidirectional RNN layers\n",
    "chained together in opposite directions \n",
    "and acting on the same input (:numref:`fig_birnn`).\n",
    "For the first RNN layer,\n",
    "the first input is $\\mathbf{x}_1$\n",
    "and the last input is $\\mathbf{x}_T$,\n",
    "but for the second RNN layer, \n",
    "the first input is $\\mathbf{x}_T$\n",
    "and the last input is $\\mathbf{x}_1$.\n",
    "To produce the output of this bidirectional RNN layer,\n",
    "we simply concatenate together the corresponding outputs\n",
    "of the two underlying unidirectional RNN layers. \n",
    "\n",
    "\n",
    "![Architecture of a bidirectional RNN.](../img/birnn.svg)\n",
    ":label:`fig_birnn`\n",
    "\n",
    "\n",
    "Formally for any time step $t$,\n",
    "we consider a minibatch input $\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$ \n",
    "(number of examples: $n$, number of inputs in each example: $d$) \n",
    "and let the hidden layer activation function be $\\phi$.\n",
    "In the bidirectional architecture,\n",
    "the forward and backward hidden states for this time step \n",
    "are $\\overrightarrow{\\mathbf{H}}_t  \\in \\mathbb{R}^{n \\times h}$ \n",
    "and $\\overleftarrow{\\mathbf{H}}_t  \\in \\mathbb{R}^{n \\times h}$, respectively,\n",
    "where $h$ is the number of hidden units.\n",
    "The forward and backward hidden state updates are as follows:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\overrightarrow{\\mathbf{H}}_t &= \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(f)} + \\overrightarrow{\\mathbf{H}}_{t-1} \\mathbf{W}_{hh}^{(f)}  + \\mathbf{b}_h^{(f)}),\\\\\n",
    "\\overleftarrow{\\mathbf{H}}_t &= \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(b)} + \\overleftarrow{\\mathbf{H}}_{t+1} \\mathbf{W}_{hh}^{(b)}  + \\mathbf{b}_h^{(b)}),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where the weights $\\mathbf{W}_{xh}^{(f)} \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}_{hh}^{(f)} \\in \\mathbb{R}^{h \\times h}, \\mathbf{W}_{xh}^{(b)} \\in \\mathbb{R}^{d \\times h}, \\text{ and } \\mathbf{W}_{hh}^{(b)} \\in \\mathbb{R}^{h \\times h}$, and biases $\\mathbf{b}_h^{(f)} \\in \\mathbb{R}^{1 \\times h}$ and $\\mathbf{b}_h^{(b)} \\in \\mathbb{R}^{1 \\times h}$ are all the model parameters.\n",
    "\n",
    "Next, we concatenate the forward and backward hidden states\n",
    "$\\overrightarrow{\\mathbf{H}}_t$ and $\\overleftarrow{\\mathbf{H}}_t$\n",
    "to obtain the hidden state $\\mathbf{H}_t \\in \\mathbb{R}^{n \\times 2h}$ to be fed into the output layer.\n",
    "In deep bidirectional RNNs with multiple hidden layers,\n",
    "such information is passed on as *input* to the next bidirectional layer. \n",
    "Last, the output layer computes the output \n",
    "$\\mathbf{O}_t \\in \\mathbb{R}^{n \\times q}$ (number of outputs: $q$):\n",
    "\n",
    "$$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q.$$\n",
    "\n",
    "Here, the weight matrix $\\mathbf{W}_{hq} \\in \\mathbb{R}^{2h \\times q}$ \n",
    "and the bias $\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$ \n",
    "are the model parameters of the output layer. \n",
    "While technically, the two directions can have different numbers of hidden units,\n",
    "this design choice is seldom made in practice. \n",
    "We now demonstrate a simple implementation of a bidirectional RNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ad2bc32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T22:08:09.083159Z",
     "iopub.status.busy": "2022-09-07T22:08:09.082433Z",
     "iopub.status.idle": "2022-09-07T22:08:10.975565Z",
     "shell.execute_reply": "2022-09-07T22:08:10.974623Z"
    },
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11bb0ec",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "### Implementation from Scratch\n",
    "\n",
    "To implement a bidirectional RNN from scratch, we can\n",
    "include two unidirectional `RNNScratch` instances\n",
    "with separate learnable parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34748a9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T22:08:10.979814Z",
     "iopub.status.busy": "2022-09-07T22:08:10.979236Z",
     "iopub.status.idle": "2022-09-07T22:08:10.984412Z",
     "shell.execute_reply": "2022-09-07T22:08:10.983668Z"
    },
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class BiRNNScratch(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.f_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)\n",
    "        self.b_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)\n",
    "        self.num_hiddens *= 2  # The output dimension will be doubled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baadddde",
   "metadata": {
    "origin_pos": 7
   },
   "source": [
    "States of forward and backward RNNs\n",
    "are updated separately,\n",
    "while outputs of these two RNNs are concatenated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fcfd27f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T22:08:10.987810Z",
     "iopub.status.busy": "2022-09-07T22:08:10.987379Z",
     "iopub.status.idle": "2022-09-07T22:08:10.992598Z",
     "shell.execute_reply": "2022-09-07T22:08:10.991849Z"
    },
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "@d2l.add_to_class(BiRNNScratch)\n",
    "def forward(self, inputs, Hs=None):\n",
    "    f_H, b_H = Hs if Hs is not None else (None, None)\n",
    "    f_outputs, f_H = self.f_rnn(inputs, f_H)\n",
    "    b_outputs, b_H = self.b_rnn(reversed(inputs), b_H)\n",
    "    outputs = [torch.cat((f, b), -1) for f, b in zip(f_outputs, b_outputs)]\n",
    "    return outputs, (f_H, b_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872fdc14",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "### Concise Implementation\n",
    "\n",
    "Using the high-level APIs,\n",
    "we can implement bidirectional RNNs more concisely.\n",
    "Here we take a GRU model as an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46bd120f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T22:08:10.997144Z",
     "iopub.status.busy": "2022-09-07T22:08:10.996602Z",
     "iopub.status.idle": "2022-09-07T22:08:11.001216Z",
     "shell.execute_reply": "2022-09-07T22:08:11.000421Z"
    },
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class BiGRU(d2l.RNN):\n",
    "    def __init__(self, num_inputs, num_hiddens):\n",
    "        d2l.Module.__init__(self)\n",
    "        self.save_hyperparameters()\n",
    "        self.rnn = nn.GRU(num_inputs, num_hiddens, bidirectional=True)\n",
    "        self.num_hiddens *= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498c8663",
   "metadata": {
    "origin_pos": 11
   },
   "source": [
    "## Summary\n",
    "\n",
    "In bidirectional RNNs, the hidden state for each time step is simultaneously determined by the data prior to and after the current time step. Bidirectional RNNs are mostly useful for sequence encoding and the estimation of observations given bidirectional context. Bidirectional RNNs are very costly to train due to long gradient chains.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. If the different directions use a different number of hidden units, how will the shape of $\\mathbf{H}_t$ change?\n",
    "1. Design a bidirectional RNN with multiple hidden layers.\n",
    "1. Polysemy is common in natural languages. For example, the word \"bank\" has different meanings in contexts “i went to the bank to deposit cash” and “i went to the bank to sit down”. How can we design a neural network model such that given a context sequence and a word, a vector representation of the word in the context will be returned? What type of neural architectures is preferred for handling polysemy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b2db94",
   "metadata": {
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1059)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}