{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Redes Neurais Recorrentes Modernas\n",
    ":label:`chap_modern_rnn`\n",
    "\n",
    "Introduzimos os conceitos básicos de RNNs, que podem lidar melhor com dados de sequência. Para demonstração, implementamos modelos de linguagem baseados em RNN em dados de texto. No entanto, tais técnicas podem\n",
    "não ser suficiente para os profissionais quando eles enfrentam uma ampla gama de problemas de aprendizagem de sequência\n",
    "hoje em dia.\n",
    "\n",
    "Por exemplo, um problema notável na prática é a instabilidade numérica dos RNNs. Embora tenhamos\n",
    "truques de implementação aplicados, como recorte de gradiente, esse problema pode ser aliviado ainda mais com\n",
    "designs mais sofisticados de modelos de sequência. Especificamente, os RNNs controlados são muito mais comuns na prática. Começaremos apresentando duas dessas redes amplamente utilizadas, chamadas de *gated recurrent units* (GRUs) e *long short-term memory* (LSTM). Além disso, vamos expandir o RNN\n",
    "arquitetura com uma única camada oculta indireta que foi discutida até agora. Descreveremos arquiteturas profundas com múltiplas camadas ocultas e discutiremos o projeto bidirecional com\n",
    "cálculos recorrentes para frente e para trás. Essas expansões são frequentemente adotadas em\n",
    "redes recorrentes modernas. Ao explicar essas variantes RNN, continuamos a considerar o\n",
    "mesmo problema de modelagem de linguagem apresentado no :numref:`chap_rnn`.\n",
    "\n",
    "Na verdade, a modelagem de linguagem revela apenas uma pequena fração do que o aprendizado de sequência é capaz.\n",
    "Em uma variedade de problemas de aprendizagem de sequência, como reconhecimento automático de fala, conversão de texto em fala,\n",
    "e tradução automática, tanto as entradas quanto as saídas são sequências de comprimento arbitrário. Explicar\n",
    "como ajustar este tipo de dados, tomaremos a tradução automática como exemplo e apresentaremos o\n",
    "arquitetura codificador-decodificador baseada em RNNs e busca de feixe para geração de sequência.\n",
    "\n",
    ":begin_tab:toc\n",
    " - [gru](gru.ipynb)\n",
    " - [lstm](lstm.ipynb)\n",
    " - [deep-rnn](deep-rnn.ipynb)\n",
    " - [bi-rnn](bi-rnn.ipynb)\n",
    " - [machine-translation-and-dataset](machine-translation-and-dataset.ipynb)\n",
    " - [encoder-decoder](encoder-decoder.ipynb)\n",
    " - [seq2seq](seq2seq.ipynb)\n",
    " - [beam-search](beam-search.ipynb)\n",
    ":end_tab:\n",
    "\n",
    "<!--stackedit_data:\n",
    "eyJoaXN0b3J5IjpbMzAwNTc5NDg1LDE1MDA1NDM1MDZdfQ==\n",
    "-->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}