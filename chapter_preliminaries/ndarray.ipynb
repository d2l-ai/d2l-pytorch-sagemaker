{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Manipulação de Dados\n",
    ":label:`sec_ndarray`\n",
    "\n",
    "Para fazer qualquer coisa, precisamos de alguma forma de armazenar e manipular dados.\n",
    "Geralmente, há duas coisas importantes que precisamos fazer com os dados: (i) adquirir\n",
    "eles; e (ii) processá-los assim que estiverem dentro do computador. Não há\n",
    "sentido em adquirir dados sem alguma forma de armazená-los, então vamos  brincar com dados sintéticos. Para começar, apresentamos o\n",
    "*array* $n$-dimensional, também chamado de *tensor*.\n",
    "\n",
    "Se você trabalhou com NumPy, o mais amplamente utilizado\n",
    "pacote de computação científica em Python,\n",
    "então você achará esta seção familiar.\n",
    "Não importa qual estrutura você usa,\n",
    "sua *classe de tensor* (`ndarray` em MXNet,\n",
    "`Tensor` em PyTorch e TensorFlow) é semelhante ao` ndarray` do NumPy com\n",
    "alguns recursos interessantes.\n",
    "Primeiro, a GPU é bem suportada para acelerar a computação\n",
    "enquanto o NumPy suporta apenas computação de CPU.\n",
    "Em segundo lugar, a classe tensor\n",
    "suporta diferenciação automática.\n",
    "Essas propriedades tornam a classe tensor adequada para aprendizado profundo.\n",
    "Ao longo do livro, quando dizemos tensores,\n",
    "estamos nos referindo a instâncias da classe tensorial, a menos que seja declarado de outra forma.\n",
    "\n",
    "## Iniciando\n",
    "\n",
    "Nesta seção, nosso objetivo é colocá-lo em funcionamento,\n",
    "equipando você com as ferramentas básicas de matemática e computação numérica\n",
    "que você desenvolverá conforme progride no livro.\n",
    "Não se preocupe se você lutar para grocar alguns dos\n",
    "os conceitos matemáticos ou funções de biblioteca.\n",
    "As seções a seguir revisitarão este material\n",
    "no contexto de exemplos práticos e irá afundar.\n",
    "Por outro lado, se você já tem alguma experiência\n",
    "e quiser se aprofundar no conteúdo matemático, basta pular esta seção.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "(**Para começar, importamos `torch`. Note que apesar de ser chamado PyTorch, devemos importar `torch` ao invés de `pytorch`.**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 5,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 7
   },
   "source": [
    "[**Um tensor representa uma matriz (possivelmente multidimensional) de valores numéricos.**]\n",
    "Com uma dimensão, um tensor corresponde (em matemática) a um *vetor*.\n",
    "Com duas dimensões, um tensor corresponde a uma * matriz *.\n",
    "Tensores com mais de dois eixos não possuem\n",
    "nomes matemáticos.\n",
    "\n",
    "Para começar, podemos usar `arange` para criar um vetor linha `x`\n",
    "contendo os primeiros 12 inteiros começando com 0,\n",
    "embora eles sejam criados como *float* por padrão.\n",
    "Cada um dos valores em um tensor é chamado de *elemento* do tensor.\n",
    "Por exemplo, existem 12 elementos no tensor `x`.\n",
    "A menos que especificado de outra forma, um novo tensor\n",
    "será armazenado na memória principal e designado para computação baseada em CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 11
   },
   "source": [
    "(**Podemos acessar o formato do tensor**) (~~e o número total de elementos~~) (o comprimento em cada coordenada)\n",
    "inspecionando sua propriedade `shape` .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 13
   },
   "source": [
    "Se quisermos apenas saber o número total de elementos em um tensor,\n",
    "ou seja, o produto de todos os *shapes*,\n",
    "podemos inspecionar seu tamanho.\n",
    "Porque estamos lidando com um vetor aqui,\n",
    "o único elemento de seu `shape` é idêntico ao seu tamanho.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "origin_pos": 15,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "Para [**mudar o *shape* de um tensor sem alterar\n",
    "o número de elementos ou seus valores**],\n",
    "podemos invocar a função `reshape`.\n",
    "Por exemplo, podemos transformar nosso tensor, `x`,\n",
    "de um vetor linha com forma (12,) para uma matriz com forma (3, 4).\n",
    "Este novo tensor contém exatamente os mesmos valores,\n",
    "mas os vê como uma matriz organizada em 3 linhas e 4 colunas.\n",
    "Para reiterar, embora a forma tenha mudado,\n",
    "os elementos não.\n",
    "Observe que o tamanho não é alterado pela remodelagem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "origin_pos": 18,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x.reshape(3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "A remodelação especificando manualmente todas as dimensões é desnecessária.\n",
    "Se nossa forma de destino for uma matriz com forma (altura, largura),\n",
    "então, depois de sabermos a largura, a altura é dada implicitamente.\n",
    "Por que devemos realizar a divisão nós mesmos?\n",
    "No exemplo acima, para obter uma matriz com 3 linhas,\n",
    "especificamos que deve ter 3 linhas e 4 colunas.\n",
    "Felizmente, os tensores podem calcular automaticamente uma dimensão considerando o resto.\n",
    "Invocamos esse recurso colocando `-1` para a dimensão\n",
    "que gostaríamos que os tensores inferissem automaticamente.\n",
    "No nosso caso, em vez de chamar `x.reshape (3, 4)`,\n",
    "poderíamos ter chamado equivalentemente `x.reshape (-1, 4)` ou `x.reshape (3, -1)`.\n",
    "\n",
    "Normalmente, queremos que nossas matrizes sejam inicializadas\n",
    "seja com zeros, uns, algumas outras constantes,\n",
    "ou números amostrados aleatoriamente de uma distribuição específica.\n",
    "[**Podemos criar um tensor representando um tensor com todos os elementos\n",
    "definido como 0**] (~~ou 1~~)\n",
    "e uma forma de (2, 3, 4) como a seguir:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "Da mesma forma, podemos criar tensores com cada elemento definido como 1 da seguinte maneira:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "origin_pos": 26,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "Frequentemente, queremos [**amostrar aleatoriamente os valores\n",
    "para cada elemento em um tensor**]\n",
    "de alguma distribuição de probabilidade.\n",
    "Por exemplo, quando construímos matrizes para servir\n",
    "como parâmetros em uma rede neural, vamos\n",
    "normalmente inicializar seus valores aleatoriamente.\n",
    "O fragmento a seguir cria um tensor com forma (3, 4).\n",
    "Cada um de seus elementos é amostrado aleatoriamente\n",
    "de uma distribuição gaussiana (normal) padrão\n",
    "com uma média de 0 e um desvio padrão de 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0383,  2.7221,  1.6101,  0.3270],\n",
       "        [ 1.2290,  0.3447, -0.8467, -1.8943],\n",
       "        [ 0.7013, -1.5338, -0.2593, -0.6438]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 32
   },
   "source": [
    "Podemos também [**especificar os valores exatos para cada elemento**] no tensor desejado\n",
    "fornecendo uma lista Python (ou lista de listas) contendo os valores numéricos.\n",
    "Aqui, a lista externa corresponde ao eixo 0 e a lista interna ao eixo 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "origin_pos": 34,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 4, 3],\n",
       "        [1, 2, 3, 4],\n",
       "        [4, 3, 2, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 36
   },
   "source": [
    "## Operações\n",
    "\n",
    "Este livro não é sobre engenharia de software.\n",
    "Nossos interesses não se limitam a simplesmente\n",
    "leitura e gravação de dados de/para matrizes.\n",
    "Queremos realizar operações matemáticas nessas matrizes.\n",
    "Algumas das operações mais simples e úteis\n",
    "são as operações elemento a elemento.\n",
    "Estes aplicam uma operação escalar padrão\n",
    "para cada elemento de uma matriz.\n",
    "Para funções que usam dois arrays como entradas,\n",
    "as operações elemento a elemento aplicam algum operador binário padrão\n",
    "em cada par de elementos correspondentes das duas matrizes.\n",
    "Podemos criar uma função elemento a elemento a partir de qualquer função\n",
    "que mapeia de um escalar para um escalar.\n",
    "\n",
    "Em notação matemática, denotaríamos tal\n",
    "um operador escalar *unário* (tomando uma entrada)\n",
    "pela assinatura $f: \\mathbb{R} \\rightarrow \\mathbb{R}$.\n",
    "Isso significa apenas que a função está mapeando\n",
    "de qualquer número real ($\\mathbb{R}$) para outro.\n",
    "Da mesma forma, denotamos um operador escalar *binário*\n",
    "(pegando duas entradas reais e produzindo uma saída)\n",
    "pela assinatura $f: \\mathbb{R}, \\mathbb{R} \\rightarrow \\mathbb{R}$.\n",
    "Dados quaisquer dois vetores $\\mathbf{u}$ e $\\mathbf{v}$ de mesmo *shape*, \n",
    "e um operador binário $f$, podemos produzir um vetor\n",
    "$\\mathbf{c} = F(\\mathbf{u},\\mathbf{v})$\n",
    "definindo $c_i \\gets f(u_i, v_i)$ para todos $i$,\n",
    "onde $c_i, u_i$ e $v_i$ são os elementos $i^\\mathrm{th}$\n",
    "dos vetores $\\mathbf{c}, \\mathbf{u}$, e $\\mathbf{v}$.\n",
    "Aqui, nós produzimos o valor vetorial\n",
    "$F: \\mathbb{R}^d, \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$\n",
    "*transformando* a função escalar para uma operação de vetor elemento a elemento.\n",
    "\n",
    "Os operadores aritméticos padrão comuns\n",
    "(`+`, `-`,` * `,` / `e` ** `)\n",
    "foram todos transformados em operações elemento a elemento\n",
    "para quaisquer tensores de formato idêntico de forma arbitrária.\n",
    "Podemos chamar operações elemento a elemento em quaisquer dois tensores da mesma forma.\n",
    "No exemplo a seguir, usamos vírgulas para formular uma tupla de 5 elementos,\n",
    "onde cada elemento é o resultado de uma operação elemento a elemento.\n",
    "\n",
    "### Operações\n",
    "\n",
    "[**Os operadores aritméticos padrão comuns\n",
    "(`+`, `-`,` * `,` / `e` ** `)\n",
    "foram todos transformados em operações elemento a elemento.**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "origin_pos": 38,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.,  4.,  6., 10.]),\n",
       " tensor([-1.,  0.,  2.,  6.]),\n",
       " tensor([ 2.,  4.,  8., 16.]),\n",
       " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
       " tensor([ 1.,  4., 16., 64.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x ** y  #  O ** é o operador exponenciação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 40
   },
   "source": [
    "Muitos (**mais operações podem ser aplicadas elemento a elemento**),\n",
    "incluindo operadores unários como exponenciação.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "origin_pos": 42,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 44
   },
   "source": [
    "Além de cálculos elemento a elemento,\n",
    "também podemos realizar operações de álgebra linear,\n",
    "incluindo produtos escalar de vetor e multiplicação de matrizes.\n",
    "Explicaremos as partes cruciais da álgebra linear\n",
    "(sem nenhum conhecimento prévio assumido) em :numref:`sec_linear-algebra`.\n",
    "\n",
    "\n",
    "\n",
    "Também podemos [***concatenar* vários tensores juntos,**]\n",
    "empilhando-os ponta a ponta para formar um tensor maior.\n",
    "Só precisamos fornecer uma lista de tensores\n",
    "e informar ao sistema ao longo de qual eixo concatenar.\n",
    "O exemplo abaixo mostra o que acontece quando concatenamos\n",
    "duas matrizes ao longo das linhas (eixo 0, o primeiro elemento da forma)\n",
    "vs. colunas (eixo 1, o segundo elemento da forma).\n",
    "Podemos ver que o comprimento do eixo 0 do primeiro tensor de saída ($6$)\n",
    "é a soma dos comprimentos do eixo 0 dos dois tensores de entrada ($3 + 3$);\n",
    "enquanto o comprimento do eixo 1 do segundo tensor de saída ($8$)\n",
    "é a soma dos comprimentos do eixo 1 dos dois tensores de entrada ($4 + 4$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "origin_pos": 46,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 48
   },
   "source": [
    "Às vezes, queremos [**construir um tensor binário por meio de *declarações lógicas*.**]\n",
    "Tome `X == Y` como exemplo.\n",
    "Para cada posição, se `X` e` Y` forem iguais nessa posição,\n",
    "a entrada correspondente no novo tensor assume o valor 1,\n",
    "o que significa que a declaração lógica `X == Y` é verdadeira nessa posição;\n",
    "caso contrário, essa posição assume 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "origin_pos": 49,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 50
   },
   "source": [
    "[**Somando todos os elementos no tensor**] resulta em um tensor com apenas um elemento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "origin_pos": 51,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 53
   },
   "source": [
    "## Mecanismo de *Broadcasting* \n",
    ":label:`subsec_broadcasting`\n",
    "\n",
    "Na seção acima, vimos como realizar operações elemento a elemento\n",
    "em dois tensores da mesma forma. Sob certas condições,\n",
    "mesmo quando as formas são diferentes, ainda podemos [**realizar operações elementar\n",
    "invocando o mecanismo de *Broadcasting*.**]\n",
    "Esse mecanismo funciona da seguinte maneira:\n",
    "Primeiro, expanda um ou ambos os arrays\n",
    "copiando elementos de forma adequada\n",
    "de modo que após esta transformação,\n",
    "os dois tensores têm a mesma forma.\n",
    "Em segundo lugar, execute as operações elemento a elemento\n",
    "nas matrizes resultantes.\n",
    "\n",
    "Na maioria dos casos, nós transmitimos ao longo de um eixo onde uma matriz\n",
    "inicialmente tem apenas o comprimento 1, como no exemplo a seguir:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "origin_pos": 55,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 57
   },
   "source": [
    "Uma vez que `a` e` b` são matrizes $3\\times1$ e $1\\times2$  respectivamente,\n",
    "suas formas não correspondem se quisermos adicioná-los.\n",
    "Nós transmitimos as entradas de ambas as matrizes em uma matriz $3\\times2$ maior da seguinte maneira:\n",
    "para a matriz `a` ele replica as colunas\n",
    "e para a matriz `b` ele replica as linhas\n",
    "antes de adicionar ambos os elementos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "origin_pos": 58,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 59
   },
   "source": [
    "## Indexação e Fatiamento\n",
    "\n",
    "Assim como em qualquer outro array Python, os elementos em um tensor podem ser acessados por índice.\n",
    "Como em qualquer matriz Python, o primeiro elemento tem índice 0\n",
    "e os intervalos são especificados para incluir o primeiro, mas *antes* do último elemento.\n",
    "Como nas listas padrão do Python, podemos acessar os elementos\n",
    "de acordo com sua posição relativa ao final da lista\n",
    "usando índices negativos.\n",
    "\n",
    "Assim, [**`[-1]` seleciona o último elemento e `[1: 3]`\n",
    "seleciona o segundo e o terceiro elementos**] da seguinte forma:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "origin_pos": 60,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.,  9., 10., 11.]),\n",
       " tensor([[ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-1], X[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 61,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "Além da leitura, (**também podemos escrever elementos de uma matriz especificando índices.**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "origin_pos": 63,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  9.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1, 2] = 9\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 65
   },
   "source": [
    "Se quisermos [**para atribuir a vários elementos o mesmo valor,\n",
    "simplesmente indexamos todos eles e, em seguida, atribuímos o valor a eles.**]\n",
    "Por exemplo, `[0: 2,:]` acessa a primeira e a segunda linhas,\n",
    "onde `:` leva todos os elementos ao longo do eixo 1 (coluna).\n",
    "Enquanto discutimos a indexação de matrizes,\n",
    "isso obviamente também funciona para vetores\n",
    "e para tensores de mais de 2 dimensões.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "origin_pos": 66,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12., 12., 12.],\n",
       "        [12., 12., 12., 12.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:2, :] = 12\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 68
   },
   "source": [
    "## Economizando memória\n",
    "\n",
    "[**As operações em execução podem fazer com que uma nova memória seja\n",
    "alocado aos resultados do host.**]\n",
    "Por exemplo, se escrevermos `Y = X + Y`,\n",
    "vamos desreferenciar o tensor que `Y` costumava apontar para\n",
    "e, em vez disso, aponte `Y` para a memória recém-alocada.\n",
    "No exemplo a seguir, demonstramos isso com a função `id ()` do Python,\n",
    "que nos dá o endereço exato do objeto referenciado na memória.\n",
    "Depois de executar `Y = Y + X`, descobriremos que` id (Y) `aponta para um local diferente.\n",
    "Isso ocorre porque o Python primeiro avalia `Y + X`,\n",
    "alocar nova memória para o resultado e, em seguida, torna `Y`\n",
    "aponte para este novo local na memória.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "origin_pos": 69,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y = Y + X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 70
   },
   "source": [
    "Isso pode ser indesejável por dois motivos.\n",
    "Em primeiro lugar, não queremos\n",
    "alocar memória desnecessariamente o tempo todo.\n",
    "No aprendizado de máquina, podemos ter\n",
    "centenas de megabytes de parâmetros\n",
    "e atualizar todos eles várias vezes por segundo.\n",
    "Normalmente, queremos realizar essas atualizações no local.\n",
    "Em segundo lugar, podemos apontar os mesmos parâmetros de várias variáveis.\n",
    "Se não atualizarmos no local, outras referências ainda apontarão para\n",
    "a localização da memória antiga, tornando possível para partes do nosso código\n",
    "para referenciar inadvertidamente parâmetros obsoletos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 71,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "Felizmente, (**executar operações no local**) é fácil.\n",
    "Podemos atribuir o resultado de uma operação\n",
    "para uma matriz previamente alocada com notação de fatia,\n",
    "por exemplo, `Y [:] = <expressão>`.\n",
    "Para ilustrar este conceito, primeiro criamos uma nova matriz `Z`\n",
    "com a mesma forma de outro `Y`,\n",
    "usando `zeros_like` para alocar um bloco de $0$ entradas.\n",
    ": end_tab:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "origin_pos": 74,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 140194801216832\n",
      "id(Z): 140194801216832\n"
     ]
    }
   ],
   "source": [
    "Z = torch.zeros_like(Y)\n",
    "print('id(Z):', id(Z))\n",
    "Z[:] = X + Y\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 76,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[**Se o valor de `X` não for reutilizado em cálculos subsequentes,\n",
    "também podemos usar `X [:] = X + Y` ou` X + = Y`\n",
    "para reduzir a sobrecarga de memória da operação.**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "origin_pos": 78,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(X)\n",
    "X += Y\n",
    "id(X) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 80
   },
   "source": [
    "## Conversão para outros objetos Python\n",
    "\n",
    "[**Converter para um tensor NumPy**], ou vice-versa, é fácil.\n",
    "O resultado convertido não compartilha memória.\n",
    "Este pequeno inconveniente é muito importante:\n",
    "quando você executa operações na CPU ou GPUs,\n",
    "você não quer interromper a computação, esperando para ver\n",
    "se o pacote NumPy do Python deseja fazer outra coisa\n",
    "com o mesmo pedaço de memória.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "origin_pos": 82,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.Tensor)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X.numpy()\n",
    "B = torch.tensor(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 84
   },
   "source": [
    "Para (**converter um tensor de tamanho 1 em um escalar Python**),\n",
    "podemos invocar a função `item` ou as funções integradas do Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "origin_pos": 86,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 88
   },
   "source": [
    "## Sumário\n",
    "\n",
    "* A principal interface para armazenar e manipular dados para *Deep Learning* é o tensor (array $n$ -dimensional). Ele fornece uma variedade de funcionalidades, incluindo operações matemáticas básicas, transmissão, indexação, divisão, economia de memória e conversão para outros objetos Python.\n",
    "\n",
    "\n",
    "## Exercícios\n",
    "\n",
    "\n",
    "1. Execute o código nesta seção. Altere a declaração condicional `X == Y` nesta seção para` X < Y` ou `X > Y`, e então veja que tipo de tensor você pode obter.\n",
    "1. Substitua os dois tensores que operam por elemento no mecanismo de transmissão por outras formas, por exemplo, tensores tridimensionais. O resultado é o mesmo que o esperado?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 90,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/27)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 92
   },
   "source": [
    "<!--stackedit_data:\n",
    "eyJoaXN0b3J5IjpbMTA4ODAyODc4NywxODIxNDIyNDIwLC02Nj\n",
    "UyNTk0NzYsNzkwOTIwODc3LC0xMzk2MDk1NTcxLC03NTk4MzM3\n",
    "MywxMTY5Mjg1NTgsLTE2OTYyODE0MTUsLTEzMDQ3MTU0ODBdfQ\n",
    "==\n",
    "-->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}