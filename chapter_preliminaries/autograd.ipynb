{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Diferenciação automática\n",
    ":label:`sec_autograd`\n",
    "\n",
    "Como já explicado em :numref:`sec_calculus`, a diferenciação é uma etapa crucial em quase todos os algoritmos de otimização de *Deep Learning*. Embora os cálculos para obter esses derivados sejam diretos, exigindo apenas alguns cálculos básicos, para modelos complexos, trabalhando as atualizações manualmente pode ser uma tarefa difícil (e muitas vezes sujeita a erros).\n",
    "*Frameworks* de *Deep learning* aceleram este trabalho calculando automaticamente as derivadas, ou seja, *diferenciação automática*. Na prática, com base em nosso modelo projetado o sistema constrói um *grafo computacional*, rastreando quais dados combinados por meio de quais operações produzem a saída. A diferenciação automática permite que o sistema propague gradientes posteriormente. Aqui, propagar(do Inglês *backpropagate*) significa simplesmente traçar o gráfico computacional, preencher as derivadas parciais em relação a cada parâmetro.\n",
    "\n",
    "\n",
    "## Um exemplo simples\n",
    "\n",
    "Como exemplo, digamos que estamos interessados em (**derivar a função $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$ com respeito ao vetor coluna $\\mathbf{x}$.**)\n",
    "Inicialmente criamos a variável `x` e atribuimos a ela um valor inicial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "[**Antes de calcularmos o gradiente de$y$ em relação a $\\mathbf{x}$,\n",
    "precisamos armazena-lo.**]\n",
    "É importante que não aloquemos nova memória cada vez que tomamos uma derivada em relação a um parâmetro porque costumamos atualizar os mesmos parâmetros milhares ou milhões de vezes e podemos rapidamente ficar sem memória.\n",
    "Observe que um gradiente de uma função com valor escalar com respeito a um vetor $\\mathbf{x}$ tem valor vetorial e tem a mesma forma de $\\mathbf{x}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "x.requires_grad_(True)  # Same as `x = torch.arange(4.0, requires_grad=True)`\n",
    "x.grad  # O valor padrão é None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "(**Então calcularemos $y$.**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "Uma vez que `x` é um vetor de comprimento 4,\n",
    "um produto interno de `x` e` x` é realizado,\n",
    "produzindo a saída escalar que atribuímos a `y`.\n",
    "Em seguida, [** podemos calcular automaticamente o gradiente de `y`\n",
    "com relação a cada componente de `x` **]\n",
    "chamando a função de retropropagação e imprimindo o gradiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "(**O gradiente da função $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$\n",
    "em relação a $\\mathbf{x}$ should be $4\\mathbf{x}$.**)\n",
    "Vamos verificar rapidamente se nosso gradiente desejado foi calculado corretamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "origin_pos": 18,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "[**Agora calculamos outra função de `x`.**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O PyTorch acumula os gradientes por padrão, precisamos\n",
    "# apagar os valores anteriores\n",
    "\n",
    "x.grad.zero_()\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "## Retroceder para variáveis não escalares\n",
    "\n",
    "Tecnicamente, quando `y` não é um escalar,\n",
    "a interpretação mais natural da diferenciação de um vetor `y`\n",
    "em relação a um vetor, `x` é uma matriz.\n",
    "Para `y` e` x` de ordem superior e dimensão superior,\n",
    "o resultado da diferenciação pode ser um tensor de ordem alta.\n",
    "\n",
    "No entanto, embora esses objetos mais exóticos apareçam\n",
    "em aprendizado de máquina avançado (incluindo [**em *Deep Learning***]),\n",
    "com mais frequência (**quando estamos retrocedendo um vetor,**)\n",
    "estamos tentando calcular as derivadas das funções de perda\n",
    "para cada constituinte de um *lote* de exemplos de treinamento.\n",
    "Aqui, (**nossa intenção é**) não calcular a matriz de diferenciação\n",
    "mas sim (**a soma das derivadas parciais\n",
    "calculado individualmente para cada exemplo**) no lote.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "origin_pos": 26,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invocar `backward` em um não escalar requer passar um argumento `gradient`\n",
    "# que especifica o gradiente da função diferenciada w.r.t `self`.\n",
    "# Em nosso caso, simplesmente queremos somar as derivadas parciais, assim passando\n",
    "# em um gradiente de uns é apropriado\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "# y.backward(torch.ones(len(x))) equivalente a:\n",
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "## Computação *Detaching* \n",
    "\n",
    "Às vezes, desejamos [**mover alguns cálculos\n",
    "fora do gráfico computacional registrado.**]\n",
    "Por exemplo, digamos que `y` foi calculado como uma função de` x`,\n",
    "e que subsequentemente `z` foi calculado como uma função de` y` e `x`.\n",
    "Agora, imagine que quiséssemos calcular\n",
    "o gradiente de `z` em relação a` x`,\n",
    "mas queria, por algum motivo, tratar `y` como uma constante,\n",
    "e apenas leve em consideração a função\n",
    "que `x` jogou após` y` foi calculado.\n",
    "Aqui, podemos desanexar `y` para retornar uma nova variável `u`\n",
    "que tem o mesmo valor que `y`, mas descarta qualquer informação\n",
    "sobre como `y` foi calculado no grafo computacional.\n",
    "Em outras palavras, o gradiente não fluirá de volta de `u` para `x`.\n",
    "Assim, a seguinte função de retropropagação calcula\n",
    "a derivada parcial de `z = u * x` com respeito a` x` enquanto trata `u` como uma constante,\n",
    "em vez da derivada parcial de `z = x * x * x` em relação a` x`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 32
   },
   "source": [
    "Uma vez que o cálculo de `y` foi registrado,\n",
    "podemos subsequentemente invocar a retropropagação em `y` para obter a derivada de` y = x * x` com respeito a `x`, que é` 2 * x`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "origin_pos": 34,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 36
   },
   "source": [
    "## Computando o Gradiente do *Python Control Flow*\n",
    "\n",
    "Uma vantagem de usar a diferenciação automática\n",
    "é que [**mesmo se**] construir o gráfo computacional de (**uma função\n",
    "requer muito trabalho com o uso do  *Python Control Flow***)\n",
    "(por exemplo, condicionais, loops e chamadas de função arbitrárias),\n",
    "(**ainda podemos calcular o gradiente da variável resultante.**)\n",
    "No trecho a seguir, observe que\n",
    "o número de iterações do loop `while`\n",
    "e a avaliação da instrução `if`\n",
    "ambos dependem do valor da entrada `a`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "origin_pos": 38,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 40
   },
   "source": [
    "Vamos computar o gradiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "origin_pos": 42,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 44
   },
   "source": [
    "Agora podemos analisar a função `f` definida acima.\n",
    "Observe que é linear por partes em sua entrada `a`.\n",
    "Em outras palavras, para qualquer `a` existe algum escalar constante` k`\n",
    "tal que `f (a) = k * a`, onde o valor de` k` depende da entrada `a`.\n",
    "Consequentemente, `d / a` nos permite verificar se o gradiente está correto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "origin_pos": 46,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 48
   },
   "source": [
    "## Sumário\n",
    "\n",
    "* *Frameworks* de *Deep learning* podem automatizar o cálculo de derivadas. Para usá-lo, primeiro anexamos gradientes às variáveis em relação às quais desejamos as derivadas parciais. Em seguida, registramos o cálculo de nosso valor alvo, executamos sua função para retropropagação e acessamos o gradiente resultante.\n",
    "\n",
    "\n",
    "## Exercícios\n",
    "\n",
    "1. Por que a segunda derivada é muito mais computacionalmente cara de se calcular do que a primeira derivada?\n",
    "2. Depois de executar a função de retropropagação, execute-a imediatamente novamente e veja o que acontece.\n",
    "3. No exemplo de fluxo de controle onde calculamos a derivada de `d` com respeito a `a`, o que aconteceria se mudássemos a variável `a` para um vetor ou matriz aleatória. Neste ponto, o resultado do cálculo `f (a)` não é mais um escalar. O que acontece com o resultado? Como analisamos isso?\n",
    "4. Redesenhe um exemplo para encontrar o gradiente do *Control Flow*. Execute e analise o resultado.\n",
    "5. Seja $f (x) = \\ sin (x)$. Plote $f (x)$ e $\\ frac {df (x)} {dx}$, onde o último é calculado sem explorar que $f '(x) = \\ cos (x)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 50,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/35)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 52
   },
   "source": [
    "<!--stackedit_data:\n",
    "eyJoaXN0b3J5IjpbLTI4NzgwMTM5OSw4ODQwMzIxNTksLTg2Mj\n",
    "IyMDIxOSwtMTg3MDI0NTA0NiwtMjAyMDM0OTY2NSwxMjQ4MDc0\n",
    "NTIyLDE0MDY0MzkzNDFdfQ==\n",
    "-->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}